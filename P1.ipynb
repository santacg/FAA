{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba092e2b-e50a-42a9-bffc-dd3db7f1c0aa",
   "metadata": {},
   "source": [
    "# Apartado 1 Naive-Bayes propio\n",
    "• Tabla con los resultados de la ejecución para los conjuntos de datos\n",
    "analizados (wdbc y heart). Considerar los dos tipos de particionado. Los\n",
    "resultados se refieren a las tasas de error y deben mostrarse tanto con\n",
    "la corrección de Laplace como sin ella. Se debe incluir tanto el\n",
    "promedio de error como su desviación típica. Es importante mostrar\n",
    "todos los resultados agrupados en una tabla para facilitar su evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6907a8fc-4874-4dd1-bc4d-8df02c7bee5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Particionado</th>\n",
       "      <th>Laplace</th>\n",
       "      <th>Error Promedio</th>\n",
       "      <th>Desviación Típica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>True</td>\n",
       "      <td>0.071831</td>\n",
       "      <td>0.008213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>False</td>\n",
       "      <td>0.071831</td>\n",
       "      <td>0.008213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>True</td>\n",
       "      <td>0.070315</td>\n",
       "      <td>0.031910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>False</td>\n",
       "      <td>0.070315</td>\n",
       "      <td>0.031910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>True</td>\n",
       "      <td>0.146957</td>\n",
       "      <td>0.017693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>False</td>\n",
       "      <td>0.146957</td>\n",
       "      <td>0.017693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>True</td>\n",
       "      <td>0.146110</td>\n",
       "      <td>0.053510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>False</td>\n",
       "      <td>0.146110</td>\n",
       "      <td>0.053510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dataset Particionado  Laplace  Error Promedio  Desviación Típica\n",
       "0   wdbc.csv       Simple     True        0.071831           0.008213\n",
       "1   wdbc.csv       Simple    False        0.071831           0.008213\n",
       "2   wdbc.csv      Cruzada     True        0.070315           0.031910\n",
       "3   wdbc.csv      Cruzada    False        0.070315           0.031910\n",
       "4  heart.csv       Simple     True        0.146957           0.017693\n",
       "5  heart.csv       Simple    False        0.146957           0.017693\n",
       "6  heart.csv      Cruzada     True        0.146110           0.053510\n",
       "7  heart.csv      Cruzada    False        0.146110           0.053510"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import EstrategiaParticionado\n",
    "from Datos import Datos\n",
    "from Clasificador import ClasificadorNaiveBayes\n",
    "from os import listdir\n",
    "\n",
    "resultados = []\n",
    "# Damos un valor aleatorio a la semilla para cada ejecución\n",
    "seed = random.random()\n",
    "\n",
    "# Ejecutamos cada dataset que se encuentre en la carpeta Datasets\n",
    "for archivo in listdir('Datasets/'):\n",
    "    dataset = Datos('Datasets/' + archivo)\n",
    "    \n",
    "    # Parámetros de las estrategias de particionado\n",
    "    n_ejecuciones = 5\n",
    "    n_folds = 5\n",
    "    estrategia_simple = EstrategiaParticionado.ValidacionSimple(n_ejecuciones, 0.25)\n",
    "    estrategia_cruzada = EstrategiaParticionado.ValidacionCruzada(n_folds)\n",
    "    clasificador = ClasificadorNaiveBayes(laplace=1)\n",
    "\n",
    "    # Con corrección de Laplace\n",
    "    error_simple_laplace = clasificador.validacion(estrategia_simple, dataset, clasificador, seed)\n",
    "    error_cruzada_laplace = clasificador.validacion(estrategia_cruzada, dataset, clasificador)\n",
    "    \n",
    "    # Sin corrección de Laplace\n",
    "    clasificador.laplace = 0\n",
    "    error_simple = clasificador.validacion(estrategia_simple, dataset, clasificador, seed)\n",
    "    error_cruzada = clasificador.validacion(estrategia_cruzada, dataset, clasificador)\n",
    "    \n",
    "    # Calculamos los promedios y las desviaciones típicas y las almacenamos en los resutlados \n",
    "    # para posteriormente mostrarlos en una tabla\n",
    "    resultados.append({\n",
    "        'Dataset': archivo,\n",
    "        'Particionado': 'Simple',\n",
    "        'Laplace': True,\n",
    "        'Error Promedio': np.mean(error_simple_laplace),\n",
    "        'Desviación Típica': np.std(error_simple_laplace)\n",
    "    })\n",
    "    resultados.append({\n",
    "        'Dataset': archivo,\n",
    "        'Particionado': 'Simple',\n",
    "        'Laplace': False,\n",
    "        'Error Promedio': np.mean(error_simple),\n",
    "        'Desviación Típica': np.std(error_simple)\n",
    "    })\n",
    "    resultados.append({\n",
    "        'Dataset': archivo,\n",
    "        'Particionado': 'Cruzada',\n",
    "        'Laplace': True,\n",
    "        'Error Promedio': np.mean(error_cruzada_laplace),\n",
    "        'Desviación Típica': np.std(error_cruzada_laplace)\n",
    "    })\n",
    "    resultados.append({\n",
    "        'Dataset': archivo,\n",
    "        'Particionado': 'Cruzada',\n",
    "        'Laplace': False,\n",
    "        'Error Promedio': np.mean(error_cruzada),\n",
    "        'Desviación Típica': np.std(error_cruzada)\n",
    "    })\n",
    "\n",
    "# Convertimos los resultados en un dataframe\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "\n",
    "# Mostramos la tabla\n",
    "display(df_resultados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd0f58d-b2db-4679-bf26-615d968e87b0",
   "metadata": {},
   "source": [
    "• Breve análisis de los resultados anteriores. Discutir el efecto Laplace.\n",
    "Para la creación de particiones se han empleado parámetros estándar que son los que se encuentran por defecto en las versiones correspondientes de scikit learn, en concreto,\n",
    "para la validación simple se ha utilizado un porcentaje del 75% (1 - 0.25) para el tamaño de la partición de entrenamiento y para la validación cruzada se han utilizado 5 folds.\n",
    "Atendiendo a los resultados del conjunto de datos \"wdbc.csv\" podemos ver que tanto el error promedio como la desviación típica son muy bajos, indicando que la clasificación es casi perfecta teniendo una tasa de error promedio cercana al 5-7%, esto puede deberse a que los datos continuos siguen realmente una distribución Gaussiana y por lo tanto al suponer nuestro Naive Bayes una distribución Gaussiana para estos datos se obtiene este error tan bajo. Por otro lado, observamos que la corrección Laplaciana no afecta al resultado, esto se explica por el hecho de que el conjunto de datos unicamente cuenta con atributos continuos, y no aplicamos dicha corrección para atributos continuos. \n",
    "Analizando los resultados del conjunto \"heart.csv\" observamos unos ratios de error ligeramente más elevados que para el otro conjunto, esto posiblemente se pueda explicar por el hecho de que alguno de los atributos no siga una distribución Gaussiana, sin embargo, el error sigue siendo relativamente bajo rondando un 15%. En cuanto a Laplace para conjuntos de entrenamiento grandes no se observa ninguna diferencia significativa puesto que no hay datos que falten en el entrenamiento y Laplace no tiene efecto, sin embargo, si reducimos significativamente el tamaño del conjunto de entrenamiento podemos ver una diferencia alrededor del 2% entre aplicar el suavizado o no, siendo el NB con suavizado más preciso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e14bd7-0ed8-47aa-8244-afdabf040c20",
   "metadata": {},
   "source": [
    "# Apartado 2 Naive-Bayes Scikit-Learn\n",
    "• Tabla de resultados equivalente a la anterior, pero utilizando los\n",
    "métodos del paquete scikit-learn: MultinomialNB, GaussianNB y\n",
    "CategoricalNB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c43e9fbf-c331-41ef-b024-435c86659a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yaserjafar/anaconda3/lib/python3.11/site-packages/sklearn/naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
      "  warnings.warn(\n",
      "/Users/yaserjafar/anaconda3/lib/python3.11/site-packages/sklearn/naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
      "  warnings.warn(\n",
      "/Users/yaserjafar/anaconda3/lib/python3.11/site-packages/sklearn/naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
      "  warnings.warn(\n",
      "/Users/yaserjafar/anaconda3/lib/python3.11/site-packages/sklearn/naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
      "  warnings.warn(\n",
      "/Users/yaserjafar/anaconda3/lib/python3.11/site-packages/sklearn/naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
      "  warnings.warn(\n",
      "/Users/yaserjafar/anaconda3/lib/python3.11/site-packages/sklearn/naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
      "  warnings.warn(\n",
      "/Users/yaserjafar/anaconda3/lib/python3.11/site-packages/sklearn/naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
      "  warnings.warn(\n",
      "/Users/yaserjafar/anaconda3/lib/python3.11/site-packages/sklearn/naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
      "  warnings.warn(\n",
      "/Users/yaserjafar/anaconda3/lib/python3.11/site-packages/sklearn/naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
      "  warnings.warn(\n",
      "/Users/yaserjafar/anaconda3/lib/python3.11/site-packages/sklearn/naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
      "  warnings.warn(\n",
      "/Users/yaserjafar/anaconda3/lib/python3.11/site-packages/sklearn/naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
      "  warnings.warn(\n",
      "/Users/yaserjafar/anaconda3/lib/python3.11/site-packages/sklearn/naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
      "  warnings.warn(\n",
      "/Users/yaserjafar/anaconda3/lib/python3.11/site-packages/sklearn/naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
      "  warnings.warn(\n",
      "/Users/yaserjafar/anaconda3/lib/python3.11/site-packages/sklearn/naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
      "  warnings.warn(\n",
      "/Users/yaserjafar/anaconda3/lib/python3.11/site-packages/sklearn/naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
      "  warnings.warn(\n",
      "/Users/yaserjafar/anaconda3/lib/python3.11/site-packages/sklearn/naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
      "  warnings.warn(\n",
      "/Users/yaserjafar/anaconda3/lib/python3.11/site-packages/sklearn/naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
      "  warnings.warn(\n",
      "/Users/yaserjafar/anaconda3/lib/python3.11/site-packages/sklearn/naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
      "  warnings.warn(\n",
      "/Users/yaserjafar/anaconda3/lib/python3.11/site-packages/sklearn/naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
      "  warnings.warn(\n",
      "/Users/yaserjafar/anaconda3/lib/python3.11/site-packages/sklearn/naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Particionado</th>\n",
       "      <th>NB</th>\n",
       "      <th>Laplace</th>\n",
       "      <th>Error Promedio</th>\n",
       "      <th>Desviación Típica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Gaussian y Categorical</td>\n",
       "      <td>True</td>\n",
       "      <td>0.062937</td>\n",
       "      <td>0.017129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Gaussian y Categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>0.062937</td>\n",
       "      <td>0.017129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>Gaussian y Categorical</td>\n",
       "      <td>True</td>\n",
       "      <td>0.061481</td>\n",
       "      <td>0.014586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>Gaussian y Categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>0.061481</td>\n",
       "      <td>0.014586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Gaussian y Multinomial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062937</td>\n",
       "      <td>0.017129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>Gaussian y Multinomial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.061481</td>\n",
       "      <td>0.014586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Gaussian y Categorical</td>\n",
       "      <td>True</td>\n",
       "      <td>0.197549</td>\n",
       "      <td>0.010866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Gaussian y Categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>0.197154</td>\n",
       "      <td>0.010527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>Gaussian y Categorical</td>\n",
       "      <td>True</td>\n",
       "      <td>0.212530</td>\n",
       "      <td>0.050886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>Gaussian y Categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>0.213073</td>\n",
       "      <td>0.050688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Gaussian y Multinomial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.193202</td>\n",
       "      <td>0.015758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>Gaussian y Multinomial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.211443</td>\n",
       "      <td>0.049689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Dataset Particionado                      NB Laplace  Error Promedio  \\\n",
       "0    wdbc.csv       Simple  Gaussian y Categorical    True        0.062937   \n",
       "1    wdbc.csv       Simple  Gaussian y Categorical   False        0.062937   \n",
       "2    wdbc.csv      Cruzada  Gaussian y Categorical    True        0.061481   \n",
       "3    wdbc.csv      Cruzada  Gaussian y Categorical   False        0.061481   \n",
       "4    wdbc.csv       Simple  Gaussian y Multinomial     NaN        0.062937   \n",
       "5    wdbc.csv      Cruzada  Gaussian y Multinomial     NaN        0.061481   \n",
       "6   heart.csv       Simple  Gaussian y Categorical    True        0.197549   \n",
       "7   heart.csv       Simple  Gaussian y Categorical   False        0.197154   \n",
       "8   heart.csv      Cruzada  Gaussian y Categorical    True        0.212530   \n",
       "9   heart.csv      Cruzada  Gaussian y Categorical   False        0.213073   \n",
       "10  heart.csv       Simple  Gaussian y Multinomial     NaN        0.193202   \n",
       "11  heart.csv      Cruzada  Gaussian y Multinomial     NaN        0.211443   \n",
       "\n",
       "    Desviación Típica  \n",
       "0            0.017129  \n",
       "1            0.017129  \n",
       "2            0.014586  \n",
       "3            0.014586  \n",
       "4            0.017129  \n",
       "5            0.014586  \n",
       "6            0.010866  \n",
       "7            0.010527  \n",
       "8            0.050886  \n",
       "9            0.050688  \n",
       "10           0.015758  \n",
       "11           0.049689  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from sklearn import model_selection\n",
    "from sklearn import naive_bayes as nb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import neighbors as knn\n",
    "\n",
    "def validacion_simple(datos_categoricos, datos_numericos, nb_gaussian, nb_categorical,\n",
    "           test_num, test_cat, test_y):\n",
    "    if datos_categoricos is not None and datos_numericos is not None:\n",
    "        error_num = 1 - nb_gaussian.score(test_num, test_y)\n",
    "        error_cat = 1 - nb_categorical.score(test_cat, test_y)\n",
    "\n",
    "        peso_num = datos_numericos.shape[1] / (datos_numericos.shape[1] + datos_categoricos.shape[1])\n",
    "        peso_cat = datos_categoricos.shape[1] / (datos_numericos.shape[1] + datos_categoricos.shape[1])\n",
    "\n",
    "        error_promedio = (error_num * peso_num) + (error_cat * peso_cat)\n",
    "    elif datos_categoricos is None and datos_numericos is not None:\n",
    "        error_num = 1 - nb_gaussian.score(test_num, test_y)\n",
    "        error_promedio = error_num\n",
    "    elif datos_categoricos is not None and datos_numericos is None:\n",
    "        error_cat = 1 - nb_categorical.score(test_cat, test_y)\n",
    "        error_promedio = error_cat\n",
    "    else:\n",
    "        error_promedio = -1\n",
    "\n",
    "    return error_promedio\n",
    "\n",
    "def validacion_cruzada(datos_categoricos_codificados, datos_numericos, nb_gaussian, nb_categorical,\n",
    "                      target): \n",
    "    if datos_categoricos_codificados is not None and datos_numericos is not None:\n",
    "        error_num = 1 - \\\n",
    "            model_selection.cross_val_score(\n",
    "                nb_gaussian, datos_numericos, target, cv=5)\n",
    "        error_cat = 1 - \\\n",
    "            model_selection.cross_val_score(\n",
    "                nb_categorical, datos_categoricos_codificados, target, cv=5)\n",
    "        error_promedio = (error_num + error_cat) / 2\n",
    "    elif datos_categoricos_codificados is None and datos_numericos is not None:\n",
    "        error_num = 1 - \\\n",
    "            model_selection.cross_val_score(\n",
    "                nb_gaussian, datos_numericos, target, cv=5)\n",
    "        error_promedio = error_num\n",
    "    elif datos_categoricos_codificados is not None and datos_numericos is None:\n",
    "        error_cat = 1 - \\\n",
    "            model_selection.cross_val_score(\n",
    "                nb_categorical, datos_categoricos_codificados, target, cv=5)\n",
    "        error_promedio = error_cat\n",
    "    else:\n",
    "        error_promedio = -1\n",
    "\n",
    "    return error_promedio\n",
    "\n",
    "resultados = []\n",
    "\n",
    "for archivo in listdir('Datasets/'):\n",
    "    dataset = 'Datasets/' + archivo\n",
    "    df = pd.read_csv(dataset, dtype={'Class': 'object'})\n",
    "\n",
    "    # Separamos características numéricas y categóricas\n",
    "    datos_numericos = df.select_dtypes(include='number')\n",
    "    datos_categoricos = df.select_dtypes(include='object')\n",
    "\n",
    "    # Separamos la columna target\n",
    "    target = df['Class']\n",
    "    datos_categoricos = datos_categoricos.drop('Class', axis=1)\n",
    "\n",
    "    # Codificación One-Hot para datos categóricos si existen\n",
    "    datos_categoricos_codificados = None\n",
    "    if not datos_categoricos.empty:\n",
    "        datos_categoricos_codificados = pd.get_dummies(\n",
    "            datos_categoricos, drop_first=True)\n",
    "    else:\n",
    "        datos_categoricos = None\n",
    "\n",
    "    # Si no hay datos numericos None\n",
    "    if datos_numericos.empty:\n",
    "        datos_numericos = None\n",
    "\n",
    "    # Concatenamos los datos numéricos y categóricos codificados\n",
    "    X = pd.concat([datos_numericos, datos_categoricos_codificados], axis=1)\n",
    "\n",
    "    n_ejecuciones = 5\n",
    "    lista_errores_simple_laplace_cat = []\n",
    "    lista_errores_simple_cat = []\n",
    "    lista_errores_simple_mult = []\n",
    "\n",
    "    nb_gaussian = nb.GaussianNB()\n",
    "    nb_categorical_laplace = nb.CategoricalNB()\n",
    "    nb_categorical = nb.CategoricalNB(alpha=0)\n",
    "    nb_multinomial = nb.MultinomialNB()\n",
    "    # Realizamos la división de los datos\n",
    "    for i in range(n_ejecuciones):\n",
    "        train_X, test_X, train_y, test_y = model_selection.train_test_split(\n",
    "            X, target, test_size=0.25)\n",
    "    \n",
    "        # Ahora separamos los datos numéricos y categóricos a partir de los conjuntos de entrenamiento y prueba\n",
    "        train_num = None\n",
    "        test_num = None\n",
    "        if datos_numericos is not None:\n",
    "            train_num = train_X[datos_numericos.columns]\n",
    "            test_num = test_X[datos_numericos.columns]\n",
    "    \n",
    "        train_cat = None\n",
    "        test_cat = None\n",
    "        if datos_categoricos_codificados is not None:\n",
    "            train_cat = train_X[datos_categoricos_codificados.columns]\n",
    "            test_cat = test_X[datos_categoricos_codificados.columns]\n",
    "    \n",
    "        # Naive Bayes para atributos numéricos\n",
    "        if datos_numericos is not None:\n",
    "            nb_gaussian.fit(train_num, train_y)\n",
    "    \n",
    "        # Naive Bayes para atributos categóricos con corrección de Laplace y sin ella\n",
    "        if datos_categoricos is not None:\n",
    "            nb_categorical_laplace.fit(train_cat, train_y)\n",
    "            nb_categorical.fit(train_cat, train_y)\n",
    "\n",
    "        # Naive Bayes multinomial\n",
    "        if datos_categoricos is not None:\n",
    "            nb_multinomial.fit(train_cat, train_y)\n",
    "    \n",
    "        # ValidaciónSimple\n",
    "        lista_errores_simple_laplace_cat.append(validacion_simple(datos_categoricos, datos_numericos, \\\n",
    "                                                 nb_gaussian, nb_categorical_laplace, test_num, test_cat, test_y))\n",
    "        lista_errores_simple_cat.append(validacion_simple(datos_categoricos, datos_numericos, \\\n",
    "                                         nb_gaussian, nb_categorical, test_num, test_cat, test_y))\n",
    "        lista_errores_simple_mult.append(validacion_simple(datos_categoricos, datos_numericos, \\\n",
    "                                         nb_gaussian, nb_multinomial, test_num, test_cat, test_y))\n",
    "\n",
    "    # ValidaciónCruzada\n",
    "    error_cruzada_laplace_cat = validacion_cruzada(datos_categoricos_codificados, datos_numericos, nb_gaussian, \\\n",
    "                                              nb_categorical_laplace, target)\n",
    "    error_cruzada_cat = validacion_cruzada(datos_categoricos_codificados, datos_numericos, nb_gaussian, \\\n",
    "                                              nb_categorical, target)\n",
    "    error_cruzada_mult = validacion_cruzada(datos_categoricos_codificados, datos_numericos, nb_gaussian, \\\n",
    "                                              nb_multinomial, target)\n",
    "\n",
    "    # Calculamos los promedios y las desviaciones típicas y las almacenamos en los resutlados \n",
    "    # para posteriormente mostrarlos en una tabla\n",
    "    resultados.append({\n",
    "        'Dataset': archivo,\n",
    "        'Particionado': 'Simple',\n",
    "        'NB': 'Gaussian y Categorical',\n",
    "        'Laplace': True,\n",
    "        'Error Promedio': np.mean(lista_errores_simple_laplace_cat),\n",
    "        'Desviación Típica': np.std(lista_errores_simple_laplace_cat)\n",
    "    })\n",
    "    resultados.append({\n",
    "        'Dataset': archivo,\n",
    "        'Particionado': 'Simple',\n",
    "        'NB': 'Gaussian y Categorical',\n",
    "        'Laplace': False,\n",
    "        'Error Promedio': np.mean(lista_errores_simple_cat),\n",
    "        'Desviación Típica': np.std(lista_errores_simple_cat)\n",
    "    })\n",
    "    resultados.append({\n",
    "        'Dataset': archivo,\n",
    "        'Particionado': 'Cruzada',\n",
    "        'NB': 'Gaussian y Categorical',\n",
    "        'Laplace': True,\n",
    "        'Error Promedio': np.mean(error_cruzada_laplace_cat),\n",
    "        'Desviación Típica': np.std(error_cruzada_laplace_cat)\n",
    "    })\n",
    "    resultados.append({\n",
    "        'Dataset': archivo,\n",
    "        'Particionado': 'Cruzada',\n",
    "        'NB': 'Gaussian y Categorical',\n",
    "        'Laplace': False,\n",
    "        'Error Promedio': np.mean(error_cruzada_cat),\n",
    "        'Desviación Típica': np.std(error_cruzada_cat)\n",
    "    })\n",
    "    \n",
    "    resultados.append({\n",
    "        'Dataset': archivo,\n",
    "        'Particionado': 'Simple',\n",
    "        'NB': 'Gaussian y Multinomial',\n",
    "        'Error Promedio': np.mean(lista_errores_simple_mult),\n",
    "        'Desviación Típica': np.std(lista_errores_simple_mult)\n",
    "    })\n",
    "    resultados.append({\n",
    "        'Dataset': archivo,\n",
    "        'Particionado': 'Cruzada',\n",
    "        'NB': 'Gaussian y Multinomial',\n",
    "        'Error Promedio': np.mean(error_cruzada_mult),\n",
    "        'Desviación Típica': np.std(error_cruzada_mult)\n",
    "    })\n",
    "\n",
    "# Convertimos los resultados en un dataframe\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "\n",
    "# Mostramos la tabla\n",
    "display(df_resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0930ab5-47c5-4e33-834f-d030c3fbee7e",
   "metadata": {},
   "source": [
    "• ¿Existe algún problema con alguno de estos métodos en alguno de los\n",
    "dos ficheros? En caso afirmativo, ¿en cuál? ¿por qué? ¿cómo podría\n",
    "resolverse?\n",
    "En \"wdbc.csv\" tanto MultinomialNB como CategoricalNB presentan problemas porque el conjunto de datos contiene solo valores numéricos continuos, y estos modelos están diseñados para tratar unicamente con datos categóricos, con lo cual, la solución adecuada para \"wdbc.csv\" pasa por usar GaussianNB, el cual está diseñado para datos numéricos continuos, o discretizar las variables continuas, en este caso hemos optado por emplear GaussianNB si no existen datos categóricos, como es el caso de dicho conjunto. En \"heart.csv\" hay datos mixtos, es decir, numéricos y categóricos, por ende MultinomialNB y CategoricalNB tendrán problemas para los atributos continuos y GaussianNB para los atributos discretos o categóricos, la solución por la que se ha optado es un modelo híbrido que emplee GaussianNB para los datos continuos y CategoricalNB o MultinomialNB para los datos categóricos (se toma la media ponderada por el número de atributos categóricos y continuos del error de ambos modelos para ello)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49735764",
   "metadata": {},
   "source": [
    "# Apartado 3 K-NN Propio \n",
    "• Resultados en forma de tabla de la clasificación mediante\n",
    "vecinos próximos para los diferentes valores de vecindad en\n",
    "los conjuntos de datos propuestos y análisis de los resultados del imapcto de la K y la normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d3c2fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Particionado</th>\n",
       "      <th>K</th>\n",
       "      <th>Normalizado</th>\n",
       "      <th>Error Promedio</th>\n",
       "      <th>Desviación Típica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>1</td>\n",
       "      <td>No Normalizado</td>\n",
       "      <td>0.370435</td>\n",
       "      <td>0.018528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>1</td>\n",
       "      <td>No Normalizado</td>\n",
       "      <td>0.350790</td>\n",
       "      <td>0.049961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>5</td>\n",
       "      <td>No Normalizado</td>\n",
       "      <td>0.308696</td>\n",
       "      <td>0.017391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>5</td>\n",
       "      <td>No Normalizado</td>\n",
       "      <td>0.314938</td>\n",
       "      <td>0.055431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>11</td>\n",
       "      <td>No Normalizado</td>\n",
       "      <td>0.293043</td>\n",
       "      <td>0.012780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>11</td>\n",
       "      <td>No Normalizado</td>\n",
       "      <td>0.307294</td>\n",
       "      <td>0.063454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>21</td>\n",
       "      <td>No Normalizado</td>\n",
       "      <td>0.302609</td>\n",
       "      <td>0.017084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>21</td>\n",
       "      <td>No Normalizado</td>\n",
       "      <td>0.296359</td>\n",
       "      <td>0.047873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>1</td>\n",
       "      <td>Normalizado</td>\n",
       "      <td>0.197391</td>\n",
       "      <td>0.013072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>1</td>\n",
       "      <td>Normalizado</td>\n",
       "      <td>0.201699</td>\n",
       "      <td>0.065957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>5</td>\n",
       "      <td>Normalizado</td>\n",
       "      <td>0.162609</td>\n",
       "      <td>0.023750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>5</td>\n",
       "      <td>Normalizado</td>\n",
       "      <td>0.154811</td>\n",
       "      <td>0.049351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>11</td>\n",
       "      <td>Normalizado</td>\n",
       "      <td>0.156522</td>\n",
       "      <td>0.013749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>11</td>\n",
       "      <td>Normalizado</td>\n",
       "      <td>0.171193</td>\n",
       "      <td>0.064275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>21</td>\n",
       "      <td>Normalizado</td>\n",
       "      <td>0.158261</td>\n",
       "      <td>0.012481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>21</td>\n",
       "      <td>Normalizado</td>\n",
       "      <td>0.158084</td>\n",
       "      <td>0.050965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>1</td>\n",
       "      <td>No Normalizado</td>\n",
       "      <td>0.081690</td>\n",
       "      <td>0.021174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>1</td>\n",
       "      <td>No Normalizado</td>\n",
       "      <td>0.093122</td>\n",
       "      <td>0.034412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>5</td>\n",
       "      <td>No Normalizado</td>\n",
       "      <td>0.063380</td>\n",
       "      <td>0.021360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>5</td>\n",
       "      <td>No Normalizado</td>\n",
       "      <td>0.073793</td>\n",
       "      <td>0.036174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>11</td>\n",
       "      <td>No Normalizado</td>\n",
       "      <td>0.069014</td>\n",
       "      <td>0.019617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>11</td>\n",
       "      <td>No Normalizado</td>\n",
       "      <td>0.072023</td>\n",
       "      <td>0.045168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>21</td>\n",
       "      <td>No Normalizado</td>\n",
       "      <td>0.067606</td>\n",
       "      <td>0.022971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>21</td>\n",
       "      <td>No Normalizado</td>\n",
       "      <td>0.079025</td>\n",
       "      <td>0.062478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>1</td>\n",
       "      <td>Normalizado</td>\n",
       "      <td>0.046479</td>\n",
       "      <td>0.005634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>1</td>\n",
       "      <td>Normalizado</td>\n",
       "      <td>0.042198</td>\n",
       "      <td>0.010327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>5</td>\n",
       "      <td>Normalizado</td>\n",
       "      <td>0.033803</td>\n",
       "      <td>0.009343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>5</td>\n",
       "      <td>Normalizado</td>\n",
       "      <td>0.040413</td>\n",
       "      <td>0.017171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>11</td>\n",
       "      <td>Normalizado</td>\n",
       "      <td>0.038028</td>\n",
       "      <td>0.015810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>11</td>\n",
       "      <td>Normalizado</td>\n",
       "      <td>0.043891</td>\n",
       "      <td>0.024778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>21</td>\n",
       "      <td>Normalizado</td>\n",
       "      <td>0.047887</td>\n",
       "      <td>0.018579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>21</td>\n",
       "      <td>Normalizado</td>\n",
       "      <td>0.049154</td>\n",
       "      <td>0.039047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Dataset Particionado   K     Normalizado  Error Promedio  \\\n",
       "0   heart.csv       Simple   1  No Normalizado        0.370435   \n",
       "1   heart.csv      Cruzada   1  No Normalizado        0.350790   \n",
       "2   heart.csv       Simple   5  No Normalizado        0.308696   \n",
       "3   heart.csv      Cruzada   5  No Normalizado        0.314938   \n",
       "4   heart.csv       Simple  11  No Normalizado        0.293043   \n",
       "5   heart.csv      Cruzada  11  No Normalizado        0.307294   \n",
       "6   heart.csv       Simple  21  No Normalizado        0.302609   \n",
       "7   heart.csv      Cruzada  21  No Normalizado        0.296359   \n",
       "8   heart.csv       Simple   1     Normalizado        0.197391   \n",
       "9   heart.csv      Cruzada   1     Normalizado        0.201699   \n",
       "10  heart.csv       Simple   5     Normalizado        0.162609   \n",
       "11  heart.csv      Cruzada   5     Normalizado        0.154811   \n",
       "12  heart.csv       Simple  11     Normalizado        0.156522   \n",
       "13  heart.csv      Cruzada  11     Normalizado        0.171193   \n",
       "14  heart.csv       Simple  21     Normalizado        0.158261   \n",
       "15  heart.csv      Cruzada  21     Normalizado        0.158084   \n",
       "16   wdbc.csv       Simple   1  No Normalizado        0.081690   \n",
       "17   wdbc.csv      Cruzada   1  No Normalizado        0.093122   \n",
       "18   wdbc.csv       Simple   5  No Normalizado        0.063380   \n",
       "19   wdbc.csv      Cruzada   5  No Normalizado        0.073793   \n",
       "20   wdbc.csv       Simple  11  No Normalizado        0.069014   \n",
       "21   wdbc.csv      Cruzada  11  No Normalizado        0.072023   \n",
       "22   wdbc.csv       Simple  21  No Normalizado        0.067606   \n",
       "23   wdbc.csv      Cruzada  21  No Normalizado        0.079025   \n",
       "24   wdbc.csv       Simple   1     Normalizado        0.046479   \n",
       "25   wdbc.csv      Cruzada   1     Normalizado        0.042198   \n",
       "26   wdbc.csv       Simple   5     Normalizado        0.033803   \n",
       "27   wdbc.csv      Cruzada   5     Normalizado        0.040413   \n",
       "28   wdbc.csv       Simple  11     Normalizado        0.038028   \n",
       "29   wdbc.csv      Cruzada  11     Normalizado        0.043891   \n",
       "30   wdbc.csv       Simple  21     Normalizado        0.047887   \n",
       "31   wdbc.csv      Cruzada  21     Normalizado        0.049154   \n",
       "\n",
       "    Desviación Típica  \n",
       "0            0.018528  \n",
       "1            0.049961  \n",
       "2            0.017391  \n",
       "3            0.055431  \n",
       "4            0.012780  \n",
       "5            0.063454  \n",
       "6            0.017084  \n",
       "7            0.047873  \n",
       "8            0.013072  \n",
       "9            0.065957  \n",
       "10           0.023750  \n",
       "11           0.049351  \n",
       "12           0.013749  \n",
       "13           0.064275  \n",
       "14           0.012481  \n",
       "15           0.050965  \n",
       "16           0.021174  \n",
       "17           0.034412  \n",
       "18           0.021360  \n",
       "19           0.036174  \n",
       "20           0.019617  \n",
       "21           0.045168  \n",
       "22           0.022971  \n",
       "23           0.062478  \n",
       "24           0.005634  \n",
       "25           0.010327  \n",
       "26           0.009343  \n",
       "27           0.017171  \n",
       "28           0.015810  \n",
       "29           0.024778  \n",
       "30           0.018579  \n",
       "31           0.039047  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Datos import Datos\n",
    "import EstrategiaParticionado\n",
    "from Clasificador import ClasificadorKNN\n",
    "from os import listdir\n",
    "\n",
    "# Datasets a utilizar\n",
    "datasets = ['heart.csv', 'wdbc.csv']\n",
    "\n",
    "# Valores de K a probar\n",
    "K_values = [1, 5, 11, 21]\n",
    "\n",
    "normalizations = [False, True]\n",
    "\n",
    "# Número de ejecuciones y folds\n",
    "n_ejecuciones = 5\n",
    "n_folds = 5\n",
    "\n",
    "resultados = []\n",
    "\n",
    "for archivo in datasets:\n",
    "    dataset = Datos('Datasets/' + archivo)\n",
    "\n",
    "    for normalizado in normalizations:\n",
    "        if normalizado:\n",
    "            normalizacion = 'Normalizado'\n",
    "        else:\n",
    "            normalizacion = 'No Normalizado'\n",
    "\n",
    "        estrategia_simple = EstrategiaParticionado.ValidacionSimple(\n",
    "            n_ejecuciones, 0.25)\n",
    "        estrategia_cruzada = EstrategiaParticionado.ValidacionCruzada(n_folds)\n",
    "\n",
    "        for K in K_values:\n",
    "            # Validación Simple\n",
    "            clasificador_simple = ClasificadorKNN(\n",
    "                K=K, normalize=normalizado)\n",
    "            errores_simple = clasificador_simple.validacion(\n",
    "                estrategia_simple, dataset, clasificador_simple)\n",
    "\n",
    "            # Validación Cruzada\n",
    "            clasificador_cruzada = ClasificadorKNN(K=K, normalize=normalizado)\n",
    "            errores_cruzada = clasificador_cruzada.validacion(\n",
    "                estrategia_cruzada, dataset, clasificador_cruzada)\n",
    "\n",
    "            resultados.append({\n",
    "                'Dataset': archivo,\n",
    "                'Particionado': 'Simple',\n",
    "                'K': K,\n",
    "                'Normalizado': normalizacion,\n",
    "                'Error Promedio': np.mean(errores_simple),\n",
    "                'Desviación Típica': np.std(errores_simple)\n",
    "            })\n",
    "            resultados.append({\n",
    "                'Dataset': archivo,\n",
    "                'Particionado': 'Cruzada',\n",
    "                'K': K,\n",
    "                'Normalizado': normalizacion,\n",
    "                'Error Promedio': np.mean(errores_cruzada),\n",
    "                'Desviación Típica': np.std(errores_cruzada)\n",
    "            })\n",
    "\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "\n",
    "display(df_resultados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5317562d",
   "metadata": {},
   "source": [
    "¿Qué impacto ha tenido la Normalización? \n",
    "\n",
    "En el primer conjunto de datos, que consta de datos categóricos y numéricos, hemos podido observar una inmensa mejora en elr endimiento tras la aplicación de la estandarización. Sobre todo, podemos ver cómo el error medio disminuye en situaciones en las que el valor de la 'K' es bajo(K=1, K=5). Esta mejora se debe a que al normalizar, todas las características se ajustan a una escala común, además el cálculo de distancias se ve menos afectado por las diferencias de magnitud entre atributos.\n",
    "Para el segundo dataset, compuesto únicamente por datos numéricos continuos, observamos que la aplicación de la normalización provoca una mejor de calidad al evaluar las distancias entre instancias y por tanto, logra una mejora consistente en la precisión para todos los valores de K.\n",
    "\n",
    "Podemos concluir con que la normalización es fundamental para el rendimiento de nuestro modelo, especialmente en datasets con tipos de datos mixtos, como en heart.csv. La normalización nos otorga la capacidad de comparar de manera más justa entre atributos al reducir las diferencias más significantes por la magnitud.\n",
    "\n",
    "¿Qué impacto ha tenido el valor de la K?\n",
    "\n",
    "Para valores pequeños, como K=1 o K=5, observamos que se presenta la mayor cantidad de error en ambos datasets. Esto se debe a que para valores bajos de 'K', el modelo procede a ser más sensible al ruido y a valores atípicos. Con un 'K' pequeño, el modelo es muy sensible a pequeñas variaciones en los datos, ya que un ligero cambio en donde se encuentran los puntos, puede provcar una alteración en el vecino más próximo, cambiando la predicción de clase. En cambio, con valores más altos de K, la decisión se basa en una mayor cantidad de datos, haciendo que el modelo sea menos sensible a pequeñas perturbaciones.\n",
    "\n",
    "¿Qué impacto han tenido lso tipos de validaciones?\n",
    "\n",
    "Podemos observar que al aplicar la Validación cruzada, no solo obteníamos una ligera disminución en el error promedio, si no que también obteníamos una desviación típica mucho más alta que en validación simple. La mejora en cuanto al error se debe a que se captura la mejor represnetatividad del modelo, esto es, al promediar el error de múltiples particiones, se obtiene una estimación más precisa del error general del modelo en comparación con la validación simple, ya que se basa en una evaluación sobre múltiples combinaciones de los datos.\n",
    "\n",
    "En cuanto al aumento en la desviación, debido a que el modelo se entrena con un subconjutno de daros ligeramente diferente en cada partición, provocamos que los conjuntos de prueba y entrenamiento estén constantemente cambiando, lo que produce una mayor variabilidad en los resultados de cada partición."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
