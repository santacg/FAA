{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba092e2b-e50a-42a9-bffc-dd3db7f1c0aa",
   "metadata": {},
   "source": [
    "# Apartado 1 Naive-Bayes propio\n",
    "• Tabla con los resultados de la ejecución para los conjuntos de datos\n",
    "analizados (wdbc y heart). Considerar los dos tipos de particionado. Los\n",
    "resultados se refieren a las tasas de error y deben mostrarse tanto con\n",
    "la corrección de Laplace como sin ella. Se debe incluir tanto el\n",
    "promedio de error como su desviación típica. Es importante mostrar\n",
    "todos los resultados agrupados en una tabla para facilitar su evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6907a8fc-4874-4dd1-bc4d-8df02c7bee5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Particionado</th>\n",
       "      <th>Laplace</th>\n",
       "      <th>Error Promedio</th>\n",
       "      <th>Desviación Típica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>True</td>\n",
       "      <td>0.047887</td>\n",
       "      <td>0.017478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>False</td>\n",
       "      <td>0.047887</td>\n",
       "      <td>0.017478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>True</td>\n",
       "      <td>0.070315</td>\n",
       "      <td>0.031910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>False</td>\n",
       "      <td>0.070315</td>\n",
       "      <td>0.031910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>True</td>\n",
       "      <td>0.146957</td>\n",
       "      <td>0.028630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>False</td>\n",
       "      <td>0.147826</td>\n",
       "      <td>0.027907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>True</td>\n",
       "      <td>0.146110</td>\n",
       "      <td>0.053510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>False</td>\n",
       "      <td>0.146110</td>\n",
       "      <td>0.053510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dataset Particionado  Laplace  Error Promedio  Desviación Típica\n",
       "0   wdbc.csv       Simple     True        0.047887           0.017478\n",
       "1   wdbc.csv       Simple    False        0.047887           0.017478\n",
       "2   wdbc.csv      Cruzada     True        0.070315           0.031910\n",
       "3   wdbc.csv      Cruzada    False        0.070315           0.031910\n",
       "4  heart.csv       Simple     True        0.146957           0.028630\n",
       "5  heart.csv       Simple    False        0.147826           0.027907\n",
       "6  heart.csv      Cruzada     True        0.146110           0.053510\n",
       "7  heart.csv      Cruzada    False        0.146110           0.053510"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import EstrategiaParticionado\n",
    "from Datos import Datos\n",
    "from Clasificador import ClasificadorNaiveBayes\n",
    "from os import listdir\n",
    "\n",
    "resultados = []\n",
    "# Damos un valor aleatorio a la semilla para cada ejecución\n",
    "seed = random.random()\n",
    "\n",
    "# Ejecutamos cada dataset que se encuentre en la carpeta Datasets\n",
    "for archivo in listdir('Datasets/'):\n",
    "    dataset = Datos('Datasets/' + archivo)\n",
    "    \n",
    "    # Parámetros de las estrategias de particionado\n",
    "    n_ejecuciones = 5\n",
    "    n_folds = 5\n",
    "    estrategia_simple = EstrategiaParticionado.ValidacionSimple(n_ejecuciones, 0.25)\n",
    "    estrategia_cruzada = EstrategiaParticionado.ValidacionCruzada(n_folds)\n",
    "    nb_simple_laplace = ClasificadorNaiveBayes(laplace=1)\n",
    "    nb_cruzada_laplace = ClasificadorNaiveBayes(laplace=1)\n",
    "\n",
    "    # Con corrección de Laplace\n",
    "    error_simple_laplace = nb_simple_laplace.validacion(estrategia_simple, dataset, nb_simple_laplace, seed)\n",
    "    error_cruzada_laplace = nb_cruzada_laplace.validacion(estrategia_cruzada, dataset, nb_cruzada_laplace)\n",
    "    \n",
    "    # Sin corrección de Laplace\n",
    "    nb_simple = ClasificadorNaiveBayes(laplace=0)\n",
    "    nb_cruzada = ClasificadorNaiveBayes(laplace=0)\n",
    "    error_simple = nb_simple.validacion(estrategia_simple, dataset, nb_simple, seed)\n",
    "    error_cruzada = nb_cruzada.validacion(estrategia_cruzada, dataset, nb_cruzada)\n",
    "    \n",
    "    # Calculamos los promedios y las desviaciones típicas y las almacenamos en los resutlados \n",
    "    # para posteriormente mostrarlos en una tabla\n",
    "    resultados.append({\n",
    "        'Dataset': archivo,\n",
    "        'Particionado': 'Simple',\n",
    "        'Laplace': True,\n",
    "        'Error Promedio': np.mean(error_simple_laplace),\n",
    "        'Desviación Típica': np.std(error_simple_laplace)\n",
    "    })\n",
    "    resultados.append({\n",
    "        'Dataset': archivo,\n",
    "        'Particionado': 'Simple',\n",
    "        'Laplace': False,\n",
    "        'Error Promedio': np.mean(error_simple),\n",
    "        'Desviación Típica': np.std(error_simple)\n",
    "    })\n",
    "    resultados.append({\n",
    "        'Dataset': archivo,\n",
    "        'Particionado': 'Cruzada',\n",
    "        'Laplace': True,\n",
    "        'Error Promedio': np.mean(error_cruzada_laplace),\n",
    "        'Desviación Típica': np.std(error_cruzada_laplace)\n",
    "    })\n",
    "    resultados.append({\n",
    "        'Dataset': archivo,\n",
    "        'Particionado': 'Cruzada',\n",
    "        'Laplace': False,\n",
    "        'Error Promedio': np.mean(error_cruzada),\n",
    "        'Desviación Típica': np.std(error_cruzada)\n",
    "    })\n",
    "\n",
    "# Convertimos los resultados en un dataframe\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "\n",
    "# Mostramos la tabla\n",
    "display(df_resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd0f58d-b2db-4679-bf26-615d968e87b0",
   "metadata": {},
   "source": [
    "• Breve análisis de los resultados anteriores. Discutir el efecto Laplace.\n",
    "\n",
    "Para la creación de particiones se han empleado parámetros estándar que son los que se encuentran por defecto en las versiones correspondientes de scikit learn, en concreto,\n",
    "para la validación simple se ha utilizado un porcentaje del 75% (1 - 0.25) para el tamaño de la partición de entrenamiento y para la validación cruzada se han utilizado 5 folds.\n",
    "Atendiendo a los resultados del conjunto de datos \"wdbc.csv\" podemos ver que tanto el error promedio como la desviación típica son muy bajos, indicando que la clasificación es casi perfecta teniendo una tasa de error promedio cercana al 5-7%, esto puede deberse a que los datos continuos siguen realmente una distribución Gaussiana y por lo tanto al suponer nuestro Naive Bayes una distribución Gaussiana para estos datos se obtiene este error tan bajo. Por otro lado, observamos que la corrección Laplaciana no afecta al resultado, esto se explica por el hecho de que el conjunto de datos unicamente cuenta con atributos continuos, y no aplicamos dicha corrección para atributos continuos. \n",
    "Analizando los resultados del conjunto \"heart.csv\" observamos unos ratios de error ligeramente más elevados que para el otro conjunto, esto posiblemente se pueda explicar por el hecho de que alguno de los atributos no siga una distribución Gaussiana, sin embargo, el error sigue siendo relativamente bajo rondando un 15%. En cuanto a Laplace para conjuntos de entrenamiento grandes no se observa ninguna diferencia significativa puesto que no hay datos que falten en el entrenamiento y Laplace no tiene efecto, sin embargo, si reducimos significativamente el tamaño del conjunto de entrenamiento podemos ver una diferencia alrededor del 2% entre aplicar el suavizado o no, siendo el NB con suavizado más preciso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e14bd7-0ed8-47aa-8244-afdabf040c20",
   "metadata": {},
   "source": [
    "# Apartado 2 Naive-Bayes Scikit-Learn\n",
    "• Tabla de resultados equivalente a la anterior, pero utilizando los\n",
    "métodos del paquete scikit-learn: MultinomialNB, GaussianNB y\n",
    "CategoricalNB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c43e9fbf-c331-41ef-b024-435c86659a8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santacg/Code/Venvs/FAA_venv/lib/python3.12/site-packages/sklearn/naive_bayes.py:1504: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(smoothed_cat_count) - np.log(smoothed_class_count.reshape(-1, 1))\n",
      "/home/santacg/Code/Venvs/FAA_venv/lib/python3.12/site-packages/sklearn/naive_bayes.py:1504: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(smoothed_cat_count) - np.log(smoothed_class_count.reshape(-1, 1))\n",
      "/home/santacg/Code/Venvs/FAA_venv/lib/python3.12/site-packages/sklearn/naive_bayes.py:1504: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(smoothed_cat_count) - np.log(smoothed_class_count.reshape(-1, 1))\n",
      "/home/santacg/Code/Venvs/FAA_venv/lib/python3.12/site-packages/sklearn/naive_bayes.py:1504: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(smoothed_cat_count) - np.log(smoothed_class_count.reshape(-1, 1))\n",
      "/home/santacg/Code/Venvs/FAA_venv/lib/python3.12/site-packages/sklearn/naive_bayes.py:1504: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(smoothed_cat_count) - np.log(smoothed_class_count.reshape(-1, 1))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Particionado</th>\n",
       "      <th>NB</th>\n",
       "      <th>Error Promedio</th>\n",
       "      <th>Desviación Típica</th>\n",
       "      <th>Laplace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Gaussian</td>\n",
       "      <td>0.058741</td>\n",
       "      <td>0.014399</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>Gaussian</td>\n",
       "      <td>0.061481</td>\n",
       "      <td>0.014586</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Gaussian</td>\n",
       "      <td>0.140870</td>\n",
       "      <td>0.016175</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>Gaussian</td>\n",
       "      <td>0.168894</td>\n",
       "      <td>0.050452</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0.138261</td>\n",
       "      <td>0.020282</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0.147826</td>\n",
       "      <td>0.024748</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Multinomial</td>\n",
       "      <td>0.226087</td>\n",
       "      <td>0.033112</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>Multinomial</td>\n",
       "      <td>0.259402</td>\n",
       "      <td>0.107582</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Dataset Particionado           NB  Error Promedio  \\\n",
       "0   Datasets/wdbc.csv       Simple     Gaussian        0.058741   \n",
       "1   Datasets/wdbc.csv      Cruzada     Gaussian        0.061481   \n",
       "2  Datasets/heart.csv       Simple     Gaussian        0.140870   \n",
       "3  Datasets/heart.csv      Cruzada     Gaussian        0.168894   \n",
       "4  Datasets/heart.csv       Simple  Categorical        0.138261   \n",
       "5  Datasets/heart.csv       Simple  Categorical        0.147826   \n",
       "6  Datasets/heart.csv       Simple  Multinomial        0.226087   \n",
       "7  Datasets/heart.csv      Cruzada  Multinomial        0.259402   \n",
       "\n",
       "   Desviación Típica Laplace  \n",
       "0           0.014399     NaN  \n",
       "1           0.014586     NaN  \n",
       "2           0.016175     NaN  \n",
       "3           0.050452     NaN  \n",
       "4           0.020282    True  \n",
       "5           0.024748   False  \n",
       "6           0.033112     NaN  \n",
       "7           0.107582     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Datos import Datos\n",
    "from sklearn import model_selection\n",
    "from sklearn import naive_bayes as nb\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "def validacion_simple(nb, test_x, test_y):\n",
    "    return 1 - nb.score(test_x, test_y)\n",
    "\n",
    "def validacion_cruzada(df, nb, target):\n",
    "    return 1 - model_selection.cross_val_score(nb, df, target, cv=5)\n",
    "\n",
    "datasets = ['Datasets/wdbc.csv', 'Datasets/heart.csv']\n",
    "resultados = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    datos = Datos(dataset)\n",
    "    df = datos.datos\n",
    "    target = df['Class']\n",
    "    df = df.drop('Class', axis=1)\n",
    "\n",
    "    n_ejecuciones = 5\n",
    "    lista_errores_simple_gauss = []\n",
    "    lista_errores_simple_laplace_cat = []\n",
    "    lista_errores_simple_cat = []\n",
    "    lista_errores_simple_mult = []\n",
    "\n",
    "    if dataset == 'Datasets/wdbc.csv':\n",
    "        for _ in range(n_ejecuciones):\n",
    "            train_X, test_X, train_y, test_y = model_selection.train_test_split(df, target, test_size=0.25)\n",
    "            nb_gaussian = nb.GaussianNB()\n",
    "            nb_gaussian.fit(train_X, train_y)\n",
    "            lista_errores_simple_gauss.append(validacion_simple(nb_gaussian, test_X, test_y))\n",
    "\n",
    "        nb_gaussian = nb.GaussianNB()\n",
    "        error_cruzada_gauss = validacion_cruzada(df, nb_gaussian, target)\n",
    "        \n",
    "        resultados.append({\n",
    "            'Dataset': dataset,\n",
    "            'Particionado': 'Simple',\n",
    "            'NB': 'Gaussian',\n",
    "            'Error Promedio': np.mean(lista_errores_simple_gauss),\n",
    "            'Desviación Típica': np.std(lista_errores_simple_gauss)\n",
    "        })\n",
    "        resultados.append({\n",
    "            'Dataset': dataset,\n",
    "            'Particionado': 'Cruzada',\n",
    "            'NB': 'Gaussian',\n",
    "            'Error Promedio': np.mean(error_cruzada_gauss),\n",
    "            'Desviación Típica': np.std(error_cruzada_gauss)\n",
    "        })\n",
    "    \n",
    "    else:\n",
    "        cols_discretas = [col for idx, col in enumerate(df.columns) if not datos.nominalAtributos[idx]]\n",
    "\n",
    "        for _ in range(n_ejecuciones):\n",
    "            train_X, test_X, train_y, test_y = model_selection.train_test_split(df, target, test_size=0.25)\n",
    "\n",
    "            discretizador = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n",
    "            train_X[cols_discretas] = discretizador.fit_transform(train_X[cols_discretas])\n",
    "            test_X[cols_discretas] = discretizador.transform(test_X[cols_discretas])\n",
    "\n",
    "            nb_gaussian = nb.GaussianNB()\n",
    "            nb_categorical_laplace = nb.CategoricalNB()\n",
    "            nb_categorical = nb.CategoricalNB(alpha=0)\n",
    "            nb_multinomial = nb.MultinomialNB()\n",
    "            \n",
    "            nb_gaussian.fit(train_X, train_y)\n",
    "            lista_errores_simple_gauss.append(validacion_simple(nb_gaussian, test_X, test_y))\n",
    "            \n",
    "            nb_categorical_laplace.fit(train_X, train_y)\n",
    "            lista_errores_simple_laplace_cat.append(validacion_simple(nb_categorical_laplace, test_X, test_y))\n",
    "            \n",
    "            nb_categorical.fit(train_X, train_y)\n",
    "            lista_errores_simple_cat.append(validacion_simple(nb_categorical, test_X, test_y))\n",
    "            \n",
    "            nb_multinomial.fit(train_X, train_y)\n",
    "            lista_errores_simple_mult.append(validacion_simple(nb_multinomial, test_X, test_y))\n",
    "\n",
    "        discretizador = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "        df_discretized = df.copy()\n",
    "        df_discretized[cols_discretas] = discretizador.fit_transform(df[cols_discretas])\n",
    "        \n",
    "        nb_categorical_laplace = nb.CategoricalNB()\n",
    "        nb_categorical = nb.CategoricalNB(alpha=0)\n",
    "        nb_multinomial = nb.MultinomialNB()\n",
    "\n",
    "        error_cruzada_gauss = validacion_cruzada(df, nb.GaussianNB(), target)\n",
    "        error_cruzada_mult = validacion_cruzada(df_discretized, nb_multinomial, target)\n",
    "        \n",
    "        resultados.append({\n",
    "            'Dataset': dataset,\n",
    "            'Particionado': 'Simple',\n",
    "            'NB': 'Gaussian',\n",
    "            'Error Promedio': np.mean(lista_errores_simple_gauss),\n",
    "            'Desviación Típica': np.std(lista_errores_simple_gauss)\n",
    "        })\n",
    "        resultados.append({\n",
    "            'Dataset': dataset,\n",
    "            'Particionado': 'Cruzada',\n",
    "            'NB': 'Gaussian',\n",
    "            'Error Promedio': np.mean(error_cruzada_gauss),\n",
    "            'Desviación Típica': np.std(error_cruzada_gauss)\n",
    "        })\n",
    "        resultados.append({\n",
    "            'Dataset': dataset,\n",
    "            'Particionado': 'Simple',\n",
    "            'NB': 'Categorical',\n",
    "            'Laplace': True,\n",
    "            'Error Promedio': np.mean(lista_errores_simple_laplace_cat),\n",
    "            'Desviación Típica': np.std(lista_errores_simple_laplace_cat)\n",
    "        })\n",
    "        resultados.append({\n",
    "            'Dataset': dataset,\n",
    "            'Particionado': 'Simple',\n",
    "            'NB': 'Categorical',\n",
    "            'Laplace': False,\n",
    "            'Error Promedio': np.mean(lista_errores_simple_cat),\n",
    "            'Desviación Típica': np.std(lista_errores_simple_cat)\n",
    "        })\n",
    "        resultados.append({\n",
    "            'Dataset': dataset,\n",
    "            'Particionado': 'Simple',\n",
    "            'NB': 'Multinomial',\n",
    "            'Error Promedio': np.mean(lista_errores_simple_mult),\n",
    "            'Desviación Típica': np.std(lista_errores_simple_mult)\n",
    "        })\n",
    "        resultados.append({\n",
    "            'Dataset': dataset,\n",
    "            'Particionado': 'Cruzada',\n",
    "            'NB': 'Multinomial',\n",
    "            'Error Promedio': np.mean(error_cruzada_mult),\n",
    "            'Desviación Típica': np.std(error_cruzada_mult)\n",
    "        })\n",
    "\n",
    "# Convertir y mostrar los resultados en un DataFrame\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "display(df_resultados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0930ab5-47c5-4e33-834f-d030c3fbee7e",
   "metadata": {},
   "source": [
    "• ¿Existe algún problema con alguno de estos métodos en alguno de los\n",
    "dos ficheros? En caso afirmativo, ¿en cuál? ¿por qué? ¿cómo podría\n",
    "resolverse?\n",
    "\n",
    "En \"wdbc.csv\" tanto MultinomialNB como CategoricalNB presentan problemas porque el conjunto de datos contiene solo valores numéricos continuos, y estos modelos están diseñados para tratar unicamente con datos categóricos, con lo cual, la solución adecuada para \"wdbc.csv\" pasa por usar GaussianNB, el cual está diseñado para datos numéricos continuos, o discretizar las variables continuas, en este caso hemos optado por emplear GaussianNB si no existen datos categóricos, como es el caso de dicho conjunto. En \"heart.csv\" hay datos mixtos, es decir, numéricos y categóricos, por ende MultinomialNB y CategoricalNB tendrán problemas para los atributos continuos y GaussianNB para los atributos discretos o categóricos, la solución por la que se ha optado es "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ec87b4-67d7-4732-8bd3-73f75f018f55",
   "metadata": {},
   "source": [
    "# Apartado 3 K-NN propio\n",
    "Resultados en forma de tabla de la clasificación mediante\n",
    "vecinos próximos para los diferentes valores de vecindad en\n",
    "los conjuntos de datos propuestos. Obtener los resultados\n",
    "tanto para datos estandarizados como sin estandarizar, con el\n",
    "objetivo de justificar el rendimiento del algoritmo en base a\n",
    "estas características. Separar por tipo de validación (simple,\n",
    "cruzada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bc60829-b6d6-44f7-8a86-a38b9ff07162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Particionado</th>\n",
       "      <th>K</th>\n",
       "      <th>Normalizado</th>\n",
       "      <th>Error Promedio</th>\n",
       "      <th>Desviación Típica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.081690</td>\n",
       "      <td>0.005634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.093122</td>\n",
       "      <td>0.034412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.070423</td>\n",
       "      <td>0.012598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.073793</td>\n",
       "      <td>0.036174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>0.071831</td>\n",
       "      <td>0.008213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>0.072023</td>\n",
       "      <td>0.045168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "      <td>0.074648</td>\n",
       "      <td>0.008451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "      <td>0.079025</td>\n",
       "      <td>0.062478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.059155</td>\n",
       "      <td>0.013061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.042198</td>\n",
       "      <td>0.010327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>0.040845</td>\n",
       "      <td>0.005270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>0.040413</td>\n",
       "      <td>0.017171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "      <td>0.047887</td>\n",
       "      <td>0.005270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "      <td>0.043891</td>\n",
       "      <td>0.024778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>21</td>\n",
       "      <td>True</td>\n",
       "      <td>0.057746</td>\n",
       "      <td>0.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>21</td>\n",
       "      <td>True</td>\n",
       "      <td>0.049154</td>\n",
       "      <td>0.039047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.359130</td>\n",
       "      <td>0.025589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.352964</td>\n",
       "      <td>0.055301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.311304</td>\n",
       "      <td>0.020870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.311671</td>\n",
       "      <td>0.055539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>0.305217</td>\n",
       "      <td>0.027553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>0.306207</td>\n",
       "      <td>0.065465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.022508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "      <td>0.297446</td>\n",
       "      <td>0.045840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.208696</td>\n",
       "      <td>0.018032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.197321</td>\n",
       "      <td>0.059267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.012481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>0.165728</td>\n",
       "      <td>0.058802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "      <td>0.157391</td>\n",
       "      <td>0.016817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "      <td>0.164600</td>\n",
       "      <td>0.042864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>21</td>\n",
       "      <td>True</td>\n",
       "      <td>0.162609</td>\n",
       "      <td>0.009366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>21</td>\n",
       "      <td>True</td>\n",
       "      <td>0.159117</td>\n",
       "      <td>0.029140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Dataset Particionado   K  Normalizado  Error Promedio  \\\n",
       "0    Datasets/wdbc.csv       Simple   1        False        0.081690   \n",
       "1    Datasets/wdbc.csv      Cruzada   1        False        0.093122   \n",
       "2    Datasets/wdbc.csv       Simple   5        False        0.070423   \n",
       "3    Datasets/wdbc.csv      Cruzada   5        False        0.073793   \n",
       "4    Datasets/wdbc.csv       Simple  11        False        0.071831   \n",
       "5    Datasets/wdbc.csv      Cruzada  11        False        0.072023   \n",
       "6    Datasets/wdbc.csv       Simple  21        False        0.074648   \n",
       "7    Datasets/wdbc.csv      Cruzada  21        False        0.079025   \n",
       "8    Datasets/wdbc.csv       Simple   1         True        0.059155   \n",
       "9    Datasets/wdbc.csv      Cruzada   1         True        0.042198   \n",
       "10   Datasets/wdbc.csv       Simple   5         True        0.040845   \n",
       "11   Datasets/wdbc.csv      Cruzada   5         True        0.040413   \n",
       "12   Datasets/wdbc.csv       Simple  11         True        0.047887   \n",
       "13   Datasets/wdbc.csv      Cruzada  11         True        0.043891   \n",
       "14   Datasets/wdbc.csv       Simple  21         True        0.057746   \n",
       "15   Datasets/wdbc.csv      Cruzada  21         True        0.049154   \n",
       "16  Datasets/heart.csv       Simple   1        False        0.359130   \n",
       "17  Datasets/heart.csv      Cruzada   1        False        0.352964   \n",
       "18  Datasets/heart.csv       Simple   5        False        0.311304   \n",
       "19  Datasets/heart.csv      Cruzada   5        False        0.311671   \n",
       "20  Datasets/heart.csv       Simple  11        False        0.305217   \n",
       "21  Datasets/heart.csv      Cruzada  11        False        0.306207   \n",
       "22  Datasets/heart.csv       Simple  21        False        0.304348   \n",
       "23  Datasets/heart.csv      Cruzada  21        False        0.297446   \n",
       "24  Datasets/heart.csv       Simple   1         True        0.208696   \n",
       "25  Datasets/heart.csv      Cruzada   1         True        0.197321   \n",
       "26  Datasets/heart.csv       Simple   5         True        0.180000   \n",
       "27  Datasets/heart.csv      Cruzada   5         True        0.165728   \n",
       "28  Datasets/heart.csv       Simple  11         True        0.157391   \n",
       "29  Datasets/heart.csv      Cruzada  11         True        0.164600   \n",
       "30  Datasets/heart.csv       Simple  21         True        0.162609   \n",
       "31  Datasets/heart.csv      Cruzada  21         True        0.159117   \n",
       "\n",
       "    Desviación Típica  \n",
       "0            0.005634  \n",
       "1            0.034412  \n",
       "2            0.012598  \n",
       "3            0.036174  \n",
       "4            0.008213  \n",
       "5            0.045168  \n",
       "6            0.008451  \n",
       "7            0.062478  \n",
       "8            0.013061  \n",
       "9            0.010327  \n",
       "10           0.005270  \n",
       "11           0.017171  \n",
       "12           0.005270  \n",
       "13           0.024778  \n",
       "14           0.006900  \n",
       "15           0.039047  \n",
       "16           0.025589  \n",
       "17           0.055301  \n",
       "18           0.020870  \n",
       "19           0.055539  \n",
       "20           0.027553  \n",
       "21           0.065465  \n",
       "22           0.022508  \n",
       "23           0.045840  \n",
       "24           0.018032  \n",
       "25           0.059267  \n",
       "26           0.012481  \n",
       "27           0.058802  \n",
       "28           0.016817  \n",
       "29           0.042864  \n",
       "30           0.009366  \n",
       "31           0.029140  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Datos import Datos\n",
    "import EstrategiaParticionado\n",
    "from Clasificador import ClasificadorKNN\n",
    "from os import listdir\n",
    "\n",
    "# Valores de K a probar\n",
    "K_values = [1, 5, 11, 21]\n",
    "\n",
    "normalizations = [False, True]\n",
    "\n",
    "# Número de ejecuciones y folds\n",
    "n_ejecuciones = 5\n",
    "n_folds = 5\n",
    "\n",
    "resultados = []\n",
    "datasets = ['Datasets/wdbc.csv', 'Datasets/heart.csv']\n",
    "\n",
    "for dataset in datasets:\n",
    "    df = Datos(dataset)\n",
    "\n",
    "    for normalizado in normalizations:\n",
    "\n",
    "        estrategia_simple = EstrategiaParticionado.ValidacionSimple(\n",
    "            n_ejecuciones, 0.25)\n",
    "        estrategia_cruzada = EstrategiaParticionado.ValidacionCruzada(n_folds)\n",
    "\n",
    "        for K in K_values:\n",
    "            # Validación Simple\n",
    "            neigh_simple = ClasificadorKNN(K=K, normalize=normalizado)\n",
    "            errores_simple = neigh_simple.validacion(\n",
    "                estrategia_simple, df, neigh_simple)\n",
    "\n",
    "            # Validación Cruzada\n",
    "            neigh_cruzada = ClasificadorKNN(K=K, normalize=normalizado)\n",
    "            errores_cruzada = neigh_cruzada.validacion(\n",
    "                estrategia_cruzada, df, neigh_cruzada)\n",
    "\n",
    "            resultados.append({\n",
    "                'Dataset': dataset,\n",
    "                'Particionado': 'Simple',\n",
    "                'K': K,\n",
    "                'Normalizado': normalizado,\n",
    "                'Error Promedio': np.mean(errores_simple),\n",
    "                'Desviación Típica': np.std(errores_simple)\n",
    "            })\n",
    "            resultados.append({\n",
    "                'Dataset': dataset,\n",
    "                'Particionado': 'Cruzada',\n",
    "                'K': K,\n",
    "                'Normalizado': normalizado,\n",
    "                'Error Promedio': np.mean(errores_cruzada),\n",
    "                'Desviación Típica': np.std(errores_cruzada)\n",
    "            })\n",
    "\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "\n",
    "display(df_resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b491e75d-89ee-4e2b-93aa-c46e13994fbb",
   "metadata": {},
   "source": [
    "• Análisis de resultados, del impacto de K y de la estandarización\n",
    "\n",
    "¿Qué impacto ha tenido la Normalización?\n",
    "\n",
    "En el primer conjunto de datos, que consta de datos categóricos y numéricos, hemos podido observar una inmensa mejora en elr endimiento tras la aplicación de la estandarización. Sobre todo, podemos ver cómo el error medio disminuye en situaciones en las que el valor de la 'K' es bajo(K=1, K=5). Esta mejora se debe a que al normalizar, todas las características se ajustan a una escala común, además el cálculo de distancias se ve menos afectado por las diferencias de magnitud entre atributos. Para el segundo dataset, compuesto únicamente por datos numéricos continuos, observamos que la aplicación de la normalización provoca una mejor de calidad al evaluar las distancias entre instancias y por tanto, logra una mejora consistente en la precisión para todos los valores de K.\n",
    "\n",
    "Podemos concluir con que la normalización es fundamental para el rendimiento de nuestro modelo, especialmente en datasets con tipos de datos mixtos, como en heart.csv. La normalización nos otorga la capacidad de comparar de manera más justa entre atributos al reducir las diferencias más significantes por la magnitud.\n",
    "\n",
    "¿Qué impacto ha tenido el valor de la K?\n",
    "\n",
    "Para valores pequeños, como K=1 o K=5, observamos que se presenta la mayor cantidad de error en ambos datasets. Esto se debe a que para valores bajos de 'K', el modelo procede a ser más sensible al ruido y a valores atípicos. Con un 'K' pequeño, el modelo es muy sensible a pequeñas variaciones en los datos, ya que un ligero cambio en donde se encuentran los puntos, puede provcar una alteración en el vecino más próximo, cambiando la predicción de clase. En cambio, con valores más altos de K, la decisión se basa en una mayor cantidad de datos, haciendo que el modelo sea menos sensible a pequeñas perturbaciones.\n",
    "\n",
    "¿Qué impacto han tenido los tipos de validaciones?\n",
    "\n",
    "Podemos observar que al aplicar la Validación cruzada, no solo obteníamos una ligera disminución en el error promedio, si no que también obteníamos una desviación típica mucho más alta que en validación simple. La mejora en cuanto al error se debe a que se captura la mejor represnetatividad del modelo, esto es, al promediar el error de múltiples particiones, se obtiene una estimación más precisa del error general del modelo en comparación con la validación simple, ya que se basa en una evaluación sobre múltiples combinaciones de los datos.\n",
    "\n",
    "En cuanto al aumento en la desviación, debido a que el modelo se entrena con un subconjutno de daros ligeramente diferente en cada partición, provocamos que los conjuntos de prueba y entrenamiento estén constantemente cambiando, lo que produce una mayor variabilidad en los resultados de cada partición.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55012888-c131-4d19-b310-1a223e2fc184",
   "metadata": {},
   "source": [
    "# Apartado 4K-NN Scikit-Learn\n",
    "• Tabla de resultados equivalente a la del apartado anterior para las\n",
    "ejecuciones realizadas con la librería KNeighborsClassifier en los\n",
    "mismos valores de K, datos estandarizados y no estandarizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2dec92a-6b2a-4e70-b66f-2138e55d04ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Particionado</th>\n",
       "      <th>K</th>\n",
       "      <th>Normalizado</th>\n",
       "      <th>Error Promedio</th>\n",
       "      <th>Desviación Típica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.004423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.094892</td>\n",
       "      <td>0.023754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.075524</td>\n",
       "      <td>0.018972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.072054</td>\n",
       "      <td>0.021763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>0.060140</td>\n",
       "      <td>0.026015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>0.070300</td>\n",
       "      <td>0.027740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "      <td>0.062937</td>\n",
       "      <td>0.027264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "      <td>0.070238</td>\n",
       "      <td>0.034136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.055944</td>\n",
       "      <td>0.028320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.049231</td>\n",
       "      <td>0.016353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>0.034965</td>\n",
       "      <td>0.008846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>0.035150</td>\n",
       "      <td>0.009610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "      <td>0.023776</td>\n",
       "      <td>0.007131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "      <td>0.036889</td>\n",
       "      <td>0.020285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>21</td>\n",
       "      <td>True</td>\n",
       "      <td>0.044755</td>\n",
       "      <td>0.009486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Datasets/wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>21</td>\n",
       "      <td>True</td>\n",
       "      <td>0.047431</td>\n",
       "      <td>0.016223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.352174</td>\n",
       "      <td>0.024748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.372529</td>\n",
       "      <td>0.036769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.311304</td>\n",
       "      <td>0.012481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.336677</td>\n",
       "      <td>0.052590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>0.292174</td>\n",
       "      <td>0.019714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>0.317011</td>\n",
       "      <td>0.066733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "      <td>0.300870</td>\n",
       "      <td>0.038142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "      <td>0.315930</td>\n",
       "      <td>0.060135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.177391</td>\n",
       "      <td>0.006390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.202667</td>\n",
       "      <td>0.023156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>0.155652</td>\n",
       "      <td>0.017260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>0.174353</td>\n",
       "      <td>0.037770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.018115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "      <td>0.188501</td>\n",
       "      <td>0.034498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>21</td>\n",
       "      <td>True</td>\n",
       "      <td>0.147826</td>\n",
       "      <td>0.026802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Datasets/heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>21</td>\n",
       "      <td>True</td>\n",
       "      <td>0.168888</td>\n",
       "      <td>0.034681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Dataset Particionado   K  Normalizado  Error Promedio  \\\n",
       "0    Datasets/wdbc.csv       Simple   1        False        0.090909   \n",
       "1    Datasets/wdbc.csv      Cruzada   1        False        0.094892   \n",
       "2    Datasets/wdbc.csv       Simple   5        False        0.075524   \n",
       "3    Datasets/wdbc.csv      Cruzada   5        False        0.072054   \n",
       "4    Datasets/wdbc.csv       Simple  11        False        0.060140   \n",
       "5    Datasets/wdbc.csv      Cruzada  11        False        0.070300   \n",
       "6    Datasets/wdbc.csv       Simple  21        False        0.062937   \n",
       "7    Datasets/wdbc.csv      Cruzada  21        False        0.070238   \n",
       "8    Datasets/wdbc.csv       Simple   1         True        0.055944   \n",
       "9    Datasets/wdbc.csv      Cruzada   1         True        0.049231   \n",
       "10   Datasets/wdbc.csv       Simple   5         True        0.034965   \n",
       "11   Datasets/wdbc.csv      Cruzada   5         True        0.035150   \n",
       "12   Datasets/wdbc.csv       Simple  11         True        0.023776   \n",
       "13   Datasets/wdbc.csv      Cruzada  11         True        0.036889   \n",
       "14   Datasets/wdbc.csv       Simple  21         True        0.044755   \n",
       "15   Datasets/wdbc.csv      Cruzada  21         True        0.047431   \n",
       "16  Datasets/heart.csv       Simple   1        False        0.352174   \n",
       "17  Datasets/heart.csv      Cruzada   1        False        0.372529   \n",
       "18  Datasets/heart.csv       Simple   5        False        0.311304   \n",
       "19  Datasets/heart.csv      Cruzada   5        False        0.336677   \n",
       "20  Datasets/heart.csv       Simple  11        False        0.292174   \n",
       "21  Datasets/heart.csv      Cruzada  11        False        0.317011   \n",
       "22  Datasets/heart.csv       Simple  21        False        0.300870   \n",
       "23  Datasets/heart.csv      Cruzada  21        False        0.315930   \n",
       "24  Datasets/heart.csv       Simple   1         True        0.177391   \n",
       "25  Datasets/heart.csv      Cruzada   1         True        0.202667   \n",
       "26  Datasets/heart.csv       Simple   5         True        0.155652   \n",
       "27  Datasets/heart.csv      Cruzada   5         True        0.174353   \n",
       "28  Datasets/heart.csv       Simple  11         True        0.140000   \n",
       "29  Datasets/heart.csv      Cruzada  11         True        0.188501   \n",
       "30  Datasets/heart.csv       Simple  21         True        0.147826   \n",
       "31  Datasets/heart.csv      Cruzada  21         True        0.168888   \n",
       "\n",
       "    Desviación Típica  \n",
       "0            0.004423  \n",
       "1            0.023754  \n",
       "2            0.018972  \n",
       "3            0.021763  \n",
       "4            0.026015  \n",
       "5            0.027740  \n",
       "6            0.027264  \n",
       "7            0.034136  \n",
       "8            0.028320  \n",
       "9            0.016353  \n",
       "10           0.008846  \n",
       "11           0.009610  \n",
       "12           0.007131  \n",
       "13           0.020285  \n",
       "14           0.009486  \n",
       "15           0.016223  \n",
       "16           0.024748  \n",
       "17           0.036769  \n",
       "18           0.012481  \n",
       "19           0.052590  \n",
       "20           0.019714  \n",
       "21           0.066733  \n",
       "22           0.038142  \n",
       "23           0.060135  \n",
       "24           0.006390  \n",
       "25           0.023156  \n",
       "26           0.017260  \n",
       "27           0.037770  \n",
       "28           0.018115  \n",
       "29           0.034498  \n",
       "30           0.026802  \n",
       "31           0.034681  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Datos import Datos\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import neighbors as knn\n",
    "from os import listdir\n",
    "\n",
    "# Valores de K a probar\n",
    "K_values = [1, 5, 11, 21]\n",
    "\n",
    "normalizations = [False, True]\n",
    "\n",
    "# Número de ejecuciones y folds\n",
    "n_ejecuciones = 5\n",
    "\n",
    "resultados = []\n",
    "datasets = ['Datasets/wdbc.csv', 'Datasets/heart.csv']\n",
    "\n",
    "for dataset in datasets:\n",
    "    datos = Datos(dataset)\n",
    "    df = datos.datos\n",
    "    # Separamos la columna target\n",
    "    target = df['Class']\n",
    "    df = df.drop('Class', axis=1)\n",
    "    \n",
    "    # Separamos los datos numéricos\n",
    "    datos_numericos = df.select_dtypes(include='number').columns\n",
    "    # Aplicamos la estandarización\n",
    "    dataset_estandarizado = df.copy()\n",
    "    scaler = StandardScaler()\n",
    "    dataset_estandarizado[datos_numericos] = scaler.fit_transform(df[datos_numericos])\n",
    "    \n",
    "    for normalizado in normalizations:\n",
    "        # Usamos los datos estandarizados o sin normalizar según el caso\n",
    "        data = dataset_estandarizado if normalizado else df\n",
    "\n",
    "        for K in K_values:\n",
    "            neigh_simple = knn.KNeighborsClassifier(n_neighbors=K, metric='minkowski', p=2)\n",
    "            neigh_cruzada = knn.KNeighborsClassifier(n_neighbors=K, metric='minkowski', p=2)\n",
    "            knn_error_simple = []\n",
    "            \n",
    "            for _ in range(n_ejecuciones):\n",
    "                # Realizamos la validación simple y el entrenamiento\n",
    "                train_X, test_X, train_y, test_y = model_selection.train_test_split(\n",
    "                    data, target, test_size=0.25)\n",
    "                neigh_simple.fit(train_X, train_y)\n",
    "                knn_error_simple.append(1 - neigh_simple.score(test_X, test_y))\n",
    "\n",
    "            # Validación cruzada\n",
    "            knn_error_cruzada = 1 - model_selection.cross_val_score(neigh_cruzada, data, target, cv=5)\n",
    "            \n",
    "            resultados.append({\n",
    "                'Dataset': dataset,\n",
    "                'Particionado': 'Simple',\n",
    "                'K': K,\n",
    "                'Normalizado': normalizado,\n",
    "                'Error Promedio': np.mean(knn_error_simple),\n",
    "                'Desviación Típica': np.std(knn_error_simple)\n",
    "            })\n",
    "            resultados.append({\n",
    "                'Dataset': dataset,\n",
    "                'Particionado': 'Cruzada',\n",
    "                'K': K,\n",
    "                'Normalizado': normalizado,\n",
    "                'Error Promedio': np.mean(knn_error_cruzada),\n",
    "                'Desviación Típica': np.std(knn_error_cruzada)\n",
    "            })\n",
    "\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "\n",
    "display(df_resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afffdf7-8215-45ca-a053-49325da2f0d9",
   "metadata": {},
   "source": [
    "• Análisis de resultados, del impacto de K y de la estandarización\n",
    "\n",
    "¿Qué impacto ha tenido la Estandarización?\n",
    "\n",
    "En el conjunto de datos que contiene únicamente datos numéricos continuos, observamos una mejora significativa en la precisión del modelo al aplicar la estandarización. Podemos ver la diferencia en valores de 'K' pequeños como 1 y 5, el error promedio disminuye de manera significativa, de aproximadamente 0.08 y 0.05 a valores de 0.04 y 0.03 respectivamente. Esto se debe a que la estandarización ajusta todas las características a una escala común, eliminando los sesgos que podrían haber afectado el cálculo de distancias debido a la magnitud de los valores.\n",
    "\n",
    "En el conjunto de datos mixto, podemos observar una disminución abstante evidente en el error promedio al normalizar los datos. Es decir, para K=1, el error promedio baja de aproximadamente 0.36 a 0.18, mientras que para valores de K más altos como K=11 y K=21, el error disminuye de aproximadamente 0.29 a 0.13. \n",
    "\n",
    "Esta gran diferencia nos indica que la estandarización es enormemente benificiosa para estos tipos de datasets que contienen datos mixtos, ya que permite que el modelo evalúe las distancias de forma equilibrada entre las distintas características.\n",
    "\n",
    "¿Qué impacto ha tenido el valor de la K?\n",
    "\n",
    "Para valores de K pequeños, como K=1 y K=5, observamos un mayor error en ambos conjuntos de datos (wdbc.csv y heart.csv). Esto se debe a que, con valores bajos de K, el modelo es más sensible al ruido y a los valores atípicos. Cuando K es pequeño, el algoritmo se basa en unos pocos vecinos cercanos, lo que puede llevar a errores si esos vecinos son puntos atípicos o de clases minoritarias en la proximidad. En cambio, con valores más altos de K, como K=21, la precisión se estabiliza, especialmente en heart.csv, donde el error promedio baja a aproximadamente 0.13. Esto sugiere que valores de K más altos ayudan a suavizar las predicciones al basarse en un grupo más grande de vecinos, lo que reduce la sensibilidad a pequeñas variaciones en los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaecc3c-707f-4603-8415-aa1de08e0ae5",
   "metadata": {},
   "source": [
    "# Apartado 5 Conclusión\n",
    "Comparar y analizar a modo de resumen los resultados propios con\n",
    "los de Scikit-Learn para los dos algoritmos de aprendizaje y ambos\n",
    "conjuntos. Utiliza una tabla para comparar los resultados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
