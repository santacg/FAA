{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba092e2b-e50a-42a9-bffc-dd3db7f1c0aa",
   "metadata": {},
   "source": [
    "# Apartado 1 Naive-Bayes propio\n",
    "• Tabla con los resultados de la ejecución para los conjuntos de datos\n",
    "analizados (wdbc y heart). Considerar los dos tipos de particionado. Los\n",
    "resultados se refieren a las tasas de error y deben mostrarse tanto con\n",
    "la corrección de Laplace como sin ella. Se debe incluir tanto el\n",
    "promedio de error como su desviación típica. Es importante mostrar\n",
    "todos los resultados agrupados en una tabla para facilitar su evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6907a8fc-4874-4dd1-bc4d-8df02c7bee5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Particionado</th>\n",
       "      <th>Laplace</th>\n",
       "      <th>Error Promedio</th>\n",
       "      <th>Desviación Típica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>True</td>\n",
       "      <td>0.145217</td>\n",
       "      <td>0.025589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>False</td>\n",
       "      <td>0.145217</td>\n",
       "      <td>0.025589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>True</td>\n",
       "      <td>0.146110</td>\n",
       "      <td>0.053510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>False</td>\n",
       "      <td>0.146110</td>\n",
       "      <td>0.053510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>True</td>\n",
       "      <td>0.054930</td>\n",
       "      <td>0.022001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>False</td>\n",
       "      <td>0.054930</td>\n",
       "      <td>0.022001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>True</td>\n",
       "      <td>0.070315</td>\n",
       "      <td>0.031910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>False</td>\n",
       "      <td>0.070315</td>\n",
       "      <td>0.031910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dataset Particionado  Laplace  Error Promedio  Desviación Típica\n",
       "0  heart.csv       Simple     True        0.145217           0.025589\n",
       "1  heart.csv       Simple    False        0.145217           0.025589\n",
       "2  heart.csv      Cruzada     True        0.146110           0.053510\n",
       "3  heart.csv      Cruzada    False        0.146110           0.053510\n",
       "4   wdbc.csv       Simple     True        0.054930           0.022001\n",
       "5   wdbc.csv       Simple    False        0.054930           0.022001\n",
       "6   wdbc.csv      Cruzada     True        0.070315           0.031910\n",
       "7   wdbc.csv      Cruzada    False        0.070315           0.031910"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import EstrategiaParticionado\n",
    "from Datos import Datos\n",
    "from Clasificador import ClasificadorNaiveBayes\n",
    "from os import listdir\n",
    "\n",
    "resultados = []\n",
    "# Damos un valor aleatorio a la semilla para cada ejecución\n",
    "seed = random.random()\n",
    "\n",
    "# Ejecutamos cada dataset que se encuentre en la carpeta Datasets\n",
    "for archivo in listdir('Datasets/'):\n",
    "    dataset = Datos('Datasets/' + archivo)\n",
    "    \n",
    "    # Parámetros de las estrategias de particionado\n",
    "    n_ejecuciones = 5\n",
    "    n_folds = 5\n",
    "    estrategia_simple = EstrategiaParticionado.ValidacionSimple(n_ejecuciones, 0.25)\n",
    "    estrategia_cruzada = EstrategiaParticionado.ValidacionCruzada(n_folds)\n",
    "    clasificador = ClasificadorNaiveBayes(laplace=1)\n",
    "\n",
    "    # Con corrección de Laplace\n",
    "    error_simple_laplace = clasificador.validacion(estrategia_simple, dataset, clasificador, seed)\n",
    "    error_cruzada_laplace = clasificador.validacion(estrategia_cruzada, dataset, clasificador)\n",
    "    \n",
    "    # Sin corrección de Laplace\n",
    "    clasificador.laplace = 0\n",
    "    error_simple = clasificador.validacion(estrategia_simple, dataset, clasificador, seed)\n",
    "    error_cruzada = clasificador.validacion(estrategia_cruzada, dataset, clasificador)\n",
    "    \n",
    "    # Calculamos los promedios y las desviaciones típicas y las almacenamos en los resutlados \n",
    "    # para posteriormente mostrarlos en una tabla\n",
    "    resultados.append({\n",
    "        'Dataset': archivo,\n",
    "        'Particionado': 'Simple',\n",
    "        'Laplace': True,\n",
    "        'Error Promedio': np.mean(error_simple_laplace),\n",
    "        'Desviación Típica': np.std(error_simple_laplace)\n",
    "    })\n",
    "    resultados.append({\n",
    "        'Dataset': archivo,\n",
    "        'Particionado': 'Simple',\n",
    "        'Laplace': False,\n",
    "        'Error Promedio': np.mean(error_simple),\n",
    "        'Desviación Típica': np.std(error_simple)\n",
    "    })\n",
    "    resultados.append({\n",
    "        'Dataset': archivo,\n",
    "        'Particionado': 'Cruzada',\n",
    "        'Laplace': True,\n",
    "        'Error Promedio': np.mean(error_cruzada_laplace),\n",
    "        'Desviación Típica': np.std(error_cruzada_laplace)\n",
    "    })\n",
    "    resultados.append({\n",
    "        'Dataset': archivo,\n",
    "        'Particionado': 'Cruzada',\n",
    "        'Laplace': False,\n",
    "        'Error Promedio': np.mean(error_cruzada),\n",
    "        'Desviación Típica': np.std(error_cruzada)\n",
    "    })\n",
    "\n",
    "# Convertimos los resultados en un dataframe\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "\n",
    "# Mostramos la tabla\n",
    "display(df_resultados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd0f58d-b2db-4679-bf26-615d968e87b0",
   "metadata": {},
   "source": [
    "• Breve análisis de los resultados anteriores. Discutir el efecto Laplace.\n",
    "Para la creación de particiones se han empleado parámetros estándar que son los que se encuentran por defecto en las versiones correspondientes de scikit learn, en concreto,\n",
    "para la validación simple se ha utilizado un porcentaje del 75% (1 - 0.25) para el tamaño de la partición de entrenamiento y para la validación cruzada se han utilizado 5 folds.\n",
    "Atendiendo a los resultados del conjunto de datos \"wdbc.csv\" podemos ver que tanto el error promedio como la desviación típica son muy bajos, indicando que la clasificación es casi perfecta teniendo una tasa de error promedio cercana al 5-7%, esto puede deberse a que los datos continuos siguen realmente una distribución Gaussiana y por lo tanto al suponer nuestro Naive Bayes una distribución Gaussiana para estos datos se obtiene este error tan bajo. Por otro lado, observamos que la corrección Laplaciana no afecta al resultado, esto se explica por el hecho de que el conjunto de datos unicamente cuenta con atributos continuos, y no aplicamos dicha corrección para atributos continuos. \n",
    "Analizando los resultados del conjunto \"heart.csv\" observamos unos ratios de error ligeramente más elevados que para el otro conjunto, esto posiblemente se pueda explicar por el hecho de que alguno de los atributos no siga una distribución Gaussiana, sin embargo, el error sigue siendo relativamente bajo rondando un 15%. En cuanto a Laplace para conjuntos de entrenamiento grandes no se observa ninguna diferencia significativa puesto que no hay datos que falten en el entrenamiento y Laplace no tiene efecto, sin embargo, si reducimos significativamente el tamaño del conjunto de entrenamiento podemos ver una diferencia alrededor del 2% entre aplicar el suavizado o no, siendo el NB con suavizado más preciso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e14bd7-0ed8-47aa-8244-afdabf040c20",
   "metadata": {},
   "source": [
    "# Apartado 2 Naive-Bayes Scikit-Learn\n",
    "• Tabla de resultados equivalente a la anterior, pero utilizando los\n",
    "métodos del paquete scikit-learn: MultinomialNB, GaussianNB y\n",
    "CategoricalNB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c43e9fbf-c331-41ef-b024-435c86659a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santacg/Code/Venvs/FAA_venv/lib/python3.12/site-packages/sklearn/naive_bayes.py:1504: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(smoothed_cat_count) - np.log(smoothed_class_count.reshape(-1, 1))\n",
      "/home/santacg/Code/Venvs/FAA_venv/lib/python3.12/site-packages/sklearn/naive_bayes.py:1504: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(smoothed_cat_count) - np.log(smoothed_class_count.reshape(-1, 1))\n",
      "/home/santacg/Code/Venvs/FAA_venv/lib/python3.12/site-packages/sklearn/naive_bayes.py:1504: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(smoothed_cat_count) - np.log(smoothed_class_count.reshape(-1, 1))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Particionado</th>\n",
       "      <th>NB</th>\n",
       "      <th>Laplace</th>\n",
       "      <th>Error Promedio</th>\n",
       "      <th>Desviación Típica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Gaussian y Categorical</td>\n",
       "      <td>True</td>\n",
       "      <td>0.065065</td>\n",
       "      <td>0.007877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Gaussian y Categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>0.065065</td>\n",
       "      <td>0.007877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>Gaussian y Categorical</td>\n",
       "      <td>True</td>\n",
       "      <td>0.061481</td>\n",
       "      <td>0.014586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>Gaussian y Categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>0.061481</td>\n",
       "      <td>0.014586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Gaussian y Multinomial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.065065</td>\n",
       "      <td>0.007877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wdbc.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>Gaussian y Multinomial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.061481</td>\n",
       "      <td>0.014586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Gaussian y Categorical</td>\n",
       "      <td>True</td>\n",
       "      <td>0.212225</td>\n",
       "      <td>0.013178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Gaussian y Categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>0.215141</td>\n",
       "      <td>0.017723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>Gaussian y Categorical</td>\n",
       "      <td>True</td>\n",
       "      <td>0.212530</td>\n",
       "      <td>0.050886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>Gaussian y Categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>0.213073</td>\n",
       "      <td>0.050688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Gaussian y Multinomial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.213891</td>\n",
       "      <td>0.013026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>heart.csv</td>\n",
       "      <td>Cruzada</td>\n",
       "      <td>Gaussian y Multinomial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.211443</td>\n",
       "      <td>0.049689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Dataset Particionado                      NB Laplace  Error Promedio  \\\n",
       "0    wdbc.csv       Simple  Gaussian y Categorical    True        0.065065   \n",
       "1    wdbc.csv       Simple  Gaussian y Categorical   False        0.065065   \n",
       "2    wdbc.csv      Cruzada  Gaussian y Categorical    True        0.061481   \n",
       "3    wdbc.csv      Cruzada  Gaussian y Categorical   False        0.061481   \n",
       "4    wdbc.csv       Simple  Gaussian y Multinomial     NaN        0.065065   \n",
       "5    wdbc.csv      Cruzada  Gaussian y Multinomial     NaN        0.061481   \n",
       "6   heart.csv       Simple  Gaussian y Categorical    True        0.212225   \n",
       "7   heart.csv       Simple  Gaussian y Categorical   False        0.215141   \n",
       "8   heart.csv      Cruzada  Gaussian y Categorical    True        0.212530   \n",
       "9   heart.csv      Cruzada  Gaussian y Categorical   False        0.213073   \n",
       "10  heart.csv       Simple  Gaussian y Multinomial     NaN        0.213891   \n",
       "11  heart.csv      Cruzada  Gaussian y Multinomial     NaN        0.211443   \n",
       "\n",
       "    Desviación Típica  \n",
       "0            0.007877  \n",
       "1            0.007877  \n",
       "2            0.014586  \n",
       "3            0.014586  \n",
       "4            0.007877  \n",
       "5            0.014586  \n",
       "6            0.013178  \n",
       "7            0.017723  \n",
       "8            0.050886  \n",
       "9            0.050688  \n",
       "10           0.013026  \n",
       "11           0.049689  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from sklearn import model_selection\n",
    "from sklearn import naive_bayes as nb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import neighbors as knn\n",
    "\n",
    "def validacion_simple(datos_categoricos, datos_numericos, nb_gaussian, nb_categorical,\n",
    "           test_num, test_cat, test_y):\n",
    "    if datos_categoricos is not None and datos_numericos is not None:\n",
    "        error_num = 1 - nb_gaussian.score(test_num, test_y)\n",
    "        error_cat = 1 - nb_categorical.score(test_cat, test_y)\n",
    "\n",
    "        peso_num = datos_numericos.shape[1] / (datos_numericos.shape[1] + datos_categoricos.shape[1])\n",
    "        peso_cat = datos_categoricos.shape[1] / (datos_numericos.shape[1] + datos_categoricos.shape[1])\n",
    "\n",
    "        error_promedio = (error_num * peso_num) + (error_cat * peso_cat)\n",
    "    elif datos_categoricos is None and datos_numericos is not None:\n",
    "        error_num = 1 - nb_gaussian.score(test_num, test_y)\n",
    "        error_promedio = error_num\n",
    "    elif datos_categoricos is not None and datos_numericos is None:\n",
    "        error_cat = 1 - nb_categorical.score(test_cat, test_y)\n",
    "        error_promedio = error_cat\n",
    "    else:\n",
    "        error_promedio = -1\n",
    "\n",
    "    return error_promedio\n",
    "\n",
    "def validacion_cruzada(datos_categoricos_codificados, datos_numericos, nb_gaussian, nb_categorical,\n",
    "                      target): \n",
    "    if datos_categoricos_codificados is not None and datos_numericos is not None:\n",
    "        error_num = 1 - \\\n",
    "            model_selection.cross_val_score(\n",
    "                nb_gaussian, datos_numericos, target, cv=5)\n",
    "        error_cat = 1 - \\\n",
    "            model_selection.cross_val_score(\n",
    "                nb_categorical, datos_categoricos_codificados, target, cv=5)\n",
    "        error_promedio = (error_num + error_cat) / 2\n",
    "    elif datos_categoricos_codificados is None and datos_numericos is not None:\n",
    "        error_num = 1 - \\\n",
    "            model_selection.cross_val_score(\n",
    "                nb_gaussian, datos_numericos, target, cv=5)\n",
    "        error_promedio = error_num\n",
    "    elif datos_categoricos_codificados is not None and datos_numericos is None:\n",
    "        error_cat = 1 - \\\n",
    "            model_selection.cross_val_score(\n",
    "                nb_categorical, datos_categoricos_codificados, target, cv=5)\n",
    "        error_promedio = error_cat\n",
    "    else:\n",
    "        error_promedio = -1\n",
    "\n",
    "    return error_promedio\n",
    "\n",
    "resultados = []\n",
    "\n",
    "for archivo in listdir('Datasets/'):\n",
    "    dataset = 'Datasets/' + archivo\n",
    "    df = pd.read_csv(dataset, dtype={'Class': 'object'})\n",
    "\n",
    "    # Separamos características numéricas y categóricas\n",
    "    datos_numericos = df.select_dtypes(include='number')\n",
    "    datos_categoricos = df.select_dtypes(include='object')\n",
    "\n",
    "    # Separamos la columna target\n",
    "    target = df['Class']\n",
    "    datos_categoricos = datos_categoricos.drop('Class', axis=1)\n",
    "\n",
    "    # Codificación One-Hot para datos categóricos si existen\n",
    "    datos_categoricos_codificados = None\n",
    "    if not datos_categoricos.empty:\n",
    "        datos_categoricos_codificados = pd.get_dummies(\n",
    "            datos_categoricos, drop_first=True)\n",
    "    else:\n",
    "        datos_categoricos = None\n",
    "\n",
    "    # Si no hay datos numericos None\n",
    "    if datos_numericos.empty:\n",
    "        datos_numericos = None\n",
    "\n",
    "    # Concatenamos los datos numéricos y categóricos codificados\n",
    "    X = pd.concat([datos_numericos, datos_categoricos_codificados], axis=1)\n",
    "\n",
    "    n_ejecuciones = 5\n",
    "    lista_errores_simple_laplace_cat = []\n",
    "    lista_errores_simple_cat = []\n",
    "    lista_errores_simple_mult = []\n",
    "\n",
    "    nb_gaussian = nb.GaussianNB()\n",
    "    nb_categorical_laplace = nb.CategoricalNB()\n",
    "    nb_categorical = nb.CategoricalNB(alpha=0)\n",
    "    nb_multinomial = nb.MultinomialNB()\n",
    "    # Realizamos la división de los datos\n",
    "    for i in range(n_ejecuciones):\n",
    "        train_X, test_X, train_y, test_y = model_selection.train_test_split(\n",
    "            X, target, test_size=0.25)\n",
    "    \n",
    "        # Ahora separamos los datos numéricos y categóricos a partir de los conjuntos de entrenamiento y prueba\n",
    "        train_num = None\n",
    "        test_num = None\n",
    "        if datos_numericos is not None:\n",
    "            train_num = train_X[datos_numericos.columns]\n",
    "            test_num = test_X[datos_numericos.columns]\n",
    "    \n",
    "        train_cat = None\n",
    "        test_cat = None\n",
    "        if datos_categoricos_codificados is not None:\n",
    "            train_cat = train_X[datos_categoricos_codificados.columns]\n",
    "            test_cat = test_X[datos_categoricos_codificados.columns]\n",
    "    \n",
    "        # Naive Bayes para atributos numéricos\n",
    "        if datos_numericos is not None:\n",
    "            nb_gaussian.fit(train_num, train_y)\n",
    "    \n",
    "        # Naive Bayes para atributos categóricos con corrección de Laplace y sin ella\n",
    "        if datos_categoricos is not None:\n",
    "            nb_categorical_laplace.fit(train_cat, train_y)\n",
    "            nb_categorical.fit(train_cat, train_y)\n",
    "\n",
    "        # Naive Bayes multinomial\n",
    "        if datos_categoricos is not None:\n",
    "            nb_multinomial.fit(train_cat, train_y)\n",
    "    \n",
    "        # ValidaciónSimple\n",
    "        lista_errores_simple_laplace_cat.append(validacion_simple(datos_categoricos, datos_numericos, \\\n",
    "                                                 nb_gaussian, nb_categorical_laplace, test_num, test_cat, test_y))\n",
    "        lista_errores_simple_cat.append(validacion_simple(datos_categoricos, datos_numericos, \\\n",
    "                                         nb_gaussian, nb_categorical, test_num, test_cat, test_y))\n",
    "        lista_errores_simple_mult.append(validacion_simple(datos_categoricos, datos_numericos, \\\n",
    "                                         nb_gaussian, nb_multinomial, test_num, test_cat, test_y))\n",
    "\n",
    "    # ValidaciónCruzada\n",
    "    error_cruzada_laplace_cat = validacion_cruzada(datos_categoricos_codificados, datos_numericos, nb_gaussian, \\\n",
    "                                              nb_categorical_laplace, target)\n",
    "    error_cruzada_cat = validacion_cruzada(datos_categoricos_codificados, datos_numericos, nb_gaussian, \\\n",
    "                                              nb_categorical, target)\n",
    "    error_cruzada_mult = validacion_cruzada(datos_categoricos_codificados, datos_numericos, nb_gaussian, \\\n",
    "                                              nb_multinomial, target)\n",
    "\n",
    "    # Calculamos los promedios y las desviaciones típicas y las almacenamos en los resutlados \n",
    "    # para posteriormente mostrarlos en una tabla\n",
    "    resultados.append({\n",
    "        'Dataset': archivo,\n",
    "        'Particionado': 'Simple',\n",
    "        'NB': 'Gaussian y Categorical',\n",
    "        'Laplace': True,\n",
    "        'Error Promedio': np.mean(lista_errores_simple_laplace_cat),\n",
    "        'Desviación Típica': np.std(lista_errores_simple_laplace_cat)\n",
    "    })\n",
    "    resultados.append({\n",
    "        'Dataset': archivo,\n",
    "        'Particionado': 'Simple',\n",
    "        'NB': 'Gaussian y Categorical',\n",
    "        'Laplace': False,\n",
    "        'Error Promedio': np.mean(lista_errores_simple_cat),\n",
    "        'Desviación Típica': np.std(lista_errores_simple_cat)\n",
    "    })\n",
    "    resultados.append({\n",
    "        'Dataset': archivo,\n",
    "        'Particionado': 'Cruzada',\n",
    "        'NB': 'Gaussian y Categorical',\n",
    "        'Laplace': True,\n",
    "        'Error Promedio': np.mean(error_cruzada_laplace_cat),\n",
    "        'Desviación Típica': np.std(error_cruzada_laplace_cat)\n",
    "    })\n",
    "    resultados.append({\n",
    "        'Dataset': archivo,\n",
    "        'Particionado': 'Cruzada',\n",
    "        'NB': 'Gaussian y Categorical',\n",
    "        'Laplace': False,\n",
    "        'Error Promedio': np.mean(error_cruzada_cat),\n",
    "        'Desviación Típica': np.std(error_cruzada_cat)\n",
    "    })\n",
    "    \n",
    "    resultados.append({\n",
    "        'Dataset': archivo,\n",
    "        'Particionado': 'Simple',\n",
    "        'NB': 'Gaussian y Multinomial',\n",
    "        'Error Promedio': np.mean(lista_errores_simple_mult),\n",
    "        'Desviación Típica': np.std(lista_errores_simple_mult)\n",
    "    })\n",
    "    resultados.append({\n",
    "        'Dataset': archivo,\n",
    "        'Particionado': 'Cruzada',\n",
    "        'NB': 'Gaussian y Multinomial',\n",
    "        'Error Promedio': np.mean(error_cruzada_mult),\n",
    "        'Desviación Típica': np.std(error_cruzada_mult)\n",
    "    })\n",
    "\n",
    "# Convertimos los resultados en un dataframe\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "\n",
    "# Mostramos la tabla\n",
    "display(df_resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0930ab5-47c5-4e33-834f-d030c3fbee7e",
   "metadata": {},
   "source": [
    "• ¿Existe algún problema con alguno de estos métodos en alguno de los\n",
    "dos ficheros? En caso afirmativo, ¿en cuál? ¿por qué? ¿cómo podría\n",
    "resolverse?\n",
    "En \"wdbc.csv\" tanto MultinomialNB como CategoricalNB presentan problemas porque el conjunto de datos contiene solo valores numéricos continuos, y estos modelos están diseñados para tratar unicamente con datos categóricos, con lo cual, la solución adecuada para \"wdbc.csv\" pasa por usar GaussianNB, el cual está diseñado para datos numéricos continuos, o discretizar las variables continuas, en este caso hemos optado por emplear GaussianNB si no existen datos categóricos, como es el caso de dicho conjunto. En \"heart.csv\" hay datos mixtos, es decir, numéricos y categóricos, por ende MultinomialNB y CategoricalNB tendrán problemas para los atributos continuos y GaussianNB para los atributos discretos o categóricos, la solución por la que se ha optado es un modelo híbrido que emplee GaussianNB para los datos continuos y CategoricalNB o MultinomialNB para los datos categóricos (se toma la media ponderada por el número de atributos categóricos y continuos del error de ambos modelos para ello)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ec87b4-67d7-4732-8bd3-73f75f018f55",
   "metadata": {},
   "source": [
    "# Apartado 3 K-NN propio\n",
    "Resultados en forma de tabla de la clasificación mediante\n",
    "vecinos próximos para los diferentes valores de vecindad en\n",
    "los conjuntos de datos propuestos. Obtener los resultados\n",
    "tanto para datos estandarizados como sin estandarizar, con el\n",
    "objetivo de justificar el rendimiento del algoritmo en base a\n",
    "estas características. Separar por tipo de validación (simple,\n",
    "cruzada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc60829-b6d6-44f7-8a86-a38b9ff07162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Datos import Datos\n",
    "import EstrategiaParticionado\n",
    "from Clasificador import ClasificadorKNN\n",
    "from os import listdir\n",
    "\n",
    "# Datasets a utilizar\n",
    "datasets = ['heart.csv', 'wdbc.csv']\n",
    "\n",
    "# Valores de K a probar\n",
    "K_values = [1, 5, 11, 21]\n",
    "\n",
    "normalizations = [False, True]\n",
    "\n",
    "# Número de ejecuciones y folds\n",
    "n_ejecuciones = 5\n",
    "n_folds = 5\n",
    "\n",
    "resultados = []\n",
    "\n",
    "for archivo in datasets:\n",
    "    dataset = Datos('Datasets/' + archivo)\n",
    "\n",
    "    for normalizado in normalizations:\n",
    "        if normalizado:\n",
    "            normalizacion = 'Normalizado'\n",
    "        else:\n",
    "            normalizacion = 'No Normalizado'\n",
    "\n",
    "        estrategia_simple = EstrategiaParticionado.ValidacionSimple(\n",
    "            n_ejecuciones, 0.25)\n",
    "        estrategia_cruzada = EstrategiaParticionado.ValidacionCruzada(n_folds)\n",
    "\n",
    "        for K in K_values:\n",
    "            # Validación Simple\n",
    "            clasificador_simple = ClasificadorKNN(\n",
    "                K=K, normalize=normalizado)\n",
    "            errores_simple = clasificador_simple.validacion(\n",
    "                estrategia_simple, dataset, clasificador_simple)\n",
    "\n",
    "            # Validación Cruzada\n",
    "            clasificador_cruzada = ClasificadorKNN(K=K, normalize=normalizado)\n",
    "            errores_cruzada = clasificador_cruzada.validacion(\n",
    "                estrategia_cruzada, dataset, clasificador_cruzada)\n",
    "\n",
    "            resultados.append({\n",
    "                'Dataset': archivo,\n",
    "                'Particionado': 'Simple',\n",
    "                'K': K,\n",
    "                'Normalizado': normalizacion,\n",
    "                'Error Promedio': np.mean(errores_simple),\n",
    "                'Desviación Típica': np.std(errores_simple)\n",
    "            })\n",
    "            resultados.append({\n",
    "                'Dataset': archivo,\n",
    "                'Particionado': 'Cruzada',\n",
    "                'K': K,\n",
    "                'Normalizado': normalizacion,\n",
    "                'Error Promedio': np.mean(errores_cruzada),\n",
    "                'Desviación Típica': np.std(errores_cruzada)\n",
    "            })\n",
    "\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "\n",
    "print(df_resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351e2186-e5b5-4a84-842a-76617c150ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FAA_P1",
   "language": "python",
   "name": "faa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
